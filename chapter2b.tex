\chapter{Designing technology for music: intervention-based design}
\label{chap:ibr}
Technical writer Jaron Lanier wrote in 2010 the invention of the \textit{Musical Instrument Digital Interface}, or \textit{MIDI}. ``One day in the early 1980s, a music synthesizer designer named Dave Smith casually made up a way to represent musical notes. It was called MIDI. His approach conceived of music from a keyboard player's point of view. MIDI was made of digital patterns that represented keyboard events like ``key-down'' and ``key-up''. 

That meant it could not describe the curvy, transient expressions a singer or a saxophone player can produce. It could only describe the tile mosaic world of the keyboardist, not the watercolor world of the violin. But there was no reason for MIDI to be concerned with the whole of musical expression, since Dave only wanted to connect some synthesizers together so that you would have a larger palette of sounds while playing a single keyboard. In spite of its limitations, MIDI became the standard scheme to represent music in software. Music programs and synthesizers were designed to work with it, and it quickly proved impractical to change or dispose of all the software and hardware. MIDI became entrenched, and despite Herculean efforts to reform its on many occasions by a multi-decade-long parade of powerful international commercial, academic and professional organizations, it remains so.'' \cite{lanier2010you}

Between 2010 and 2020, technology has rapidly developed, and the state of music and audio technology does not look quite as bleak. This chapter provides an overview of developments that enable a more complex music representation than MIDI. It also examines how MIDI is still very common and how it might frame---or even limit---our thinking about music. An overarching theme is that we can value a technological invention such as MIDI as powerful tool while also considering what can be either improved or fundamentally changed. This chapter places music technology development into the framework of intervention based research (IBR), where ``confronting a theory with reality has been shown to produce knowledge'' \cite{chandrasekaran2020ibr}. The way an invention in audio technology is received and affects how musicians and listeners relate to music can lead us to re-frame the way we think about music.

\section{Intervention Based Research}
Chandrasekaran \textit{et al} \cite{chandrasekaran2020ibr} describe how IBR is conducted in the context of use of a method \textbf{M} to apply a basket of knowledge \textbf{T}\footnote{The original publication uses the term \textit{basket of theories}, but I use \textit{knowledge} to avoid confusion in later sections where I differentiate theory from other types of background knowledge that can be used to design the method.}---including theories, descriptions, assumptions, recipes, typologies, categories, and constraints---to bring a problem situation \textbf{S} to an outcome \textbf{S*}. An unexpected result or failed intervention triggers abductive reasoning, which leads to adjustments to \textbf{M} or \textbf{T}. In abductive reasoning, the designer uses his or her knowledge of the world infer the best explanation for what happened. In this flexible framework, the conclusion does not need to follow logically from the premises---as would be the case in deductive reasoning---or be based on statistical data---as would be the case in inductive reasoning. It simply needs to be the most plausible explanation. \cite{sep-abduction} Abductive reasoning provides a way to deal with complexity in real-world systems without having a fully formed theoretical understanding of how they work. Sometimes, what emerges is that \textbf{S*}---where everyone had agreed to head---turns out to be the wrong place to go. \cite{chandrasekaran2020ibr} Unexpected events reveal the ill-defined nature of the problem, leading to a reframing activity. This process can be repeated multiple times, leading to an iteration iteration between theory and evidence. 

As I frame the development of an autotuner program in the context of IBR, the question comes up of what to put into the basket of knowledge. Should I use a theory of musical intonation? Or stick to description, e.g., empirically derived information? What are my assumptions about musical intonation, and are there constraints to the scope of the program? What recipes is the program built on?

\subsection{A theory of musical intonation}
This section provides a brief overview of literature on the concept of a theory, its advantages, and its limitations. It then considers Antares Auto-Tune as being based on a theory, and explores what a theory would look like for the proposed approach, which involves psycho-cultural phenomena. 

\subsubsection{Theory: A brief overview}
\label{sec:theory}
Bacharach defines theory as a ``statement of relations among concepts within a set of boundary assumptions and constraints''. A theory helps us ``organize (parsimoniously) and communicate (clearly).'' \cite{bacharach1989organizational} Its function ``is that of preventing the observer from being dazzled by the full-blown complexity of natural or concrete events.'' \cite{hall1957theories} Furthermore, it moves beyond simple description, enabling explanation and prediction of events: It focuses on questions How, Why, When instead of What. \cite{bacharach1989organizational}. Theories can be elegant in their simplicity and explanatory power. Developing a theory, however, often involves a trade-off. Bacharach writes: ``all theories are constrained by their specific critical bounding assumptions. These assumptions include the implicit values of the theorist and the often explicit restrictions regarding space and time.'' Furthermore, a theory that generalizes well ``[require] a higher level of abstraction, which means that [it] sacrifices the level of detail needed to fit a specific situation. Similarly, Van de Ven describes how increasing a theory's internal consistency often comes at the expense of limiting its scope, quoting Poggi: ``A way of seeing is a way of not seeing.'' \cite{poggi1965main} Micro logic leads to macro nonsense \cite{van1989nothing}.

\subsubsection{Auto-Tuner's basket of knowledge}
Based on section \ref{sec:interval-conceptualization-design}, which describes in detail the assumptions behind Auto-Tune, I consider Auto-Tune to be built on the ratio theory of musical intonation. Ratio theory could be stated as: If the ratio of two notes' fundamental frequencies takes a value belonging to a set of specified values, then the interval will sound in tune. This theory is very elegant and provides a high level of abstraction. For example, it applies to all musical contexts and all timbres. However, the theory alone is not sufficient for implementing the program. As described in \ref{sec:ratio}, implementing ratio theory is often mathematically impossible. To fix this issue, Auto-Tune's basket of knowledge includes the recipe of moving from Pythagorean and Just intonation to the equal-tempered scale: a ``hack'' or compromise that makes the theories usable in practice, while sacrificing some of the purity of the intervals. Other items in Auto-Tune's basket of knowledge are the assumption that the musician wishes to be in tune as defined by ratio theory, and the constraints that the musician be using the equal-tempered scale.

\subsubsection{Basket of knowledge for the proposed approach}
I now consider what a theory would look like if basing it on empirically derived measures of intonation. The number of variables increases---or explodes. I list some possible features that might be relevant to determining whether a note is likely to sound in tune as judged by a listener. The list is not comprehensive, but provides a general idea.

The first set of features relates to the music. It includes the genre of the piece being performed (including stylistic choices common for the genre, such as pitch bending), backing track instrumentation (number of instruments, timbre, relative amplitude), global and local tempo, duration of the given note, harmonic and melodic context.

The second set of features relates to the singer. It includes vocal type (soprano, alto, tenor, bass), vocal characteristics (timbre, breathiness) fundamental frequency characteristics (vibrato rate, vibrato depth, and jitter \cite{devaney2020new}), vocal training (years, genre e.g., classical, jazz, rock), general musical training (not related to singing), vocal ability (range, stability, expressiveness), personal aesthetics, amount of vibrato, ADSR envelope of the given note, and choices made earlier in the performance.

The third set of features relates to the listener. It includes musical background (musical genre familiarity and preference, culture, musical training, hearing ability, subjective preference), audio quality (live, analog, digital, compression, sample rate, bit depth, speaker quality, number of channels), room acoustics (reverberance, distance from the source to the listener), perceived intonation of previous notes, relationship of listener to performer (self, teacher, parent, fan, audience member who bought an expensive ticket).

More generally, the theory needs to take into account what is known about the the physics of sound and psychoacoustics.

Given all of these features, many of which are hard to quantify or measure objectively, a theory is clearly challenging to develop. I consider for there to be two directions in this situation. One is to prune the set of features and simplify the problem until it is possible to build a theory, and the other is to consider an alternative to developing a theory. As I describe in the next section, I choose the latter by using description instead of theory, and develop a data-driven approach to autotuning.

\section{Stages of knowledge}
To conclude this chapter, I briefly consider how the development of the proposed deep autotuner fits into Bohn's framework for measuring and managing technological knowledge \cite{bohn1998measuring}. Bohn identifies eight stages of technological knowledge, ranging from complete ignorance to complete understanding. I summarize them here but encourage the reader to refer to the paper for more details. As I introduce each stage of knowledge, I also relate it to autotuning.

\textit{Stage One---Complete ignorance} You do not know that a phenomenon exists or that it is relevant. You cannot hear musical intonation.
\textit{Stage Two---Awareness} You know of the phenomenon but are not sure how to use it. At this point, you can start investigating it. You notice that some musical performances are consonant and others are dissonant.
\textit{Stage Three---Measure}
You can measure variables and see how they relate to the phenomenon, but not control them. You can investigate the relationship between fundamental frequency, timbre, and musical intonation.
\textit{Stage Four---Control of the mean}
You can stabilize the process you developed when variables are near the mean. You can tune the vocal track under simple conditions, such as a sustained chord. 
\textit{Stage Five---Process capability}
You can control the variables with precision across a range of values and develop a consistent ``recipe''. You develop an automatic pitch correction system.
\textit{Stage Six---Process characterization} (know how)
You know how the variable affects the result and can fine-tune your ``recipe''. You make adjustments to your autotuner.
\textit{Stage Seven---Know why}
You have a scientific model of the process. You have a theory of musical intonation. 
\textit{Stage Eight---Complete knowledge}
You know the ``complete functional form and parameter values that determine the result [...] as a function of all the inputs''. This stage is never reached in practice but can be approached asymptotically. You fully understand musical intonation in all its complexity and subjectivity. 
Bohn writes: ``High-tech manufacturing requires rapid learning about multiple variables in new products and processes. We can frame a definition in terms of the stage of knowledge: \textit{high-tech processes are those in which many of the important variables are at stage four or below}. This makes the process difficult to control and work with, so a lot of effort goes into raising the knowledge level as quickly as possible.'' {bohn1998measuring}

In order to use engineering in service of art, we need to move up and down the stages of knowledge. This thesis moves down. 

Pre-theory: can we learn useful information about intonation from other performances

- categorization of data—whether qualitative or quantitative—is not theory. 

- search for goodness of fit between empirically derived categorizations and …. Some of these studies, are often particularly rich and thus useful as grounds for theory building. 

Hambrick: - pre-theory
- Theories help us organize our thoughts, generate coherent explanations, and improve our predictions. In short, theories help us achieve understanding. But theories are not ends in themselves.
- 

Bacharach - measurements need to be specific

describe how 

\subsubsection{Pre-theory research}
\label{sec:pretheory}
Hambrick writes writes about the usefulness of research that is not ``theory-driven''. It might still be ``theoretically interesting''---if it results in development or revision of existing theories. He continues by describing how a ``theory focus prevents reporting of rich detail about interesting phenomena for which no theory yet exists.'' 

I believe this idea from social science applies to music technology. Music is not considered something that we, as humans, understand deeply, despite the mature field of music theory. This has not stopped humanity from developing musical instruments and making music. Continuing this tradition in music technology might lead to new means of musical expression, new developments in music theory, or both of the above. I argue this further in the rest of the chapter.

\section{Control, interpretability, and expressivity in music technology}
In this section, I describe how the way a music program is structured affects how much we can control it, interpret it, and how expressive it is. I describe the advantages and disadvantages of increasing each of these characteristics, and where I choose to place the proposed program. 

\subsection{Control}
Programmers usually provide a computer with exact, step-by-step instructions. The computer then executes these exactly, with no ability to deal with ambiguity. When designing music technology, a natural, naive approach is to provide the computer precise instructions for what to output under a comprehensive list of circumstances. This approach provides exact control over the output. However, formulating this set of instructions is quite similar to formulating a theory. As described in \ref{sec:theory}, this tends to result in simplifications and loss of richness. 

Developments in machine learning and deep learning have provided a framework for inserting randomness into music technology. Specifically, they utilize computers' capacity to generate pseudorandom numbers and to follow statistical distributions. They also provide a framework for computers to detect or learn patterns in data. Incorporating randomness into music technology removes some control. Music Plus One \cite{raphael2010music}, a musical accompaniment system, provides an example. The accompaniment track dynamically adjusts tempo based on a soloist's actions. The adjustments are based on real-time score matching using a hidden Markov model and future note timing prediction based on a Kalman filter-like model. Though the structure of these models is precisely determined by the programmer, and therefore controlled, the actual values adopted during a performance are out of the programmer's control. The result is much more dynamic than an alternative, where the programmer would have enforced specific tempi under specific circumstances. I note that Auto-Tune is one example of a program that provides full control and no randomness.

Other machine-learning programs provide less control than Music Plus One. Non-negative matrix factorization (NMF) \cite{LeeDD2000nips}---e.g., for magnitude spectrogram processing---is one such example. A NMF program represents a matrix as a set of basis vectors and activations. It iteratively minimizes the error between the input and reconstruction. The outcome is partially under the programmer's control, as the program enforces desired characteristics such as non-negativity. However, the model can converge to multiple different results, and this aspect cannot be pre-determined. A deep neural network (DNN) is by default even less controllable. The programmer can design the input and target data to the program and train the model to output data that is similar to the target, but has little control over the weights that the model learns. The programmer also has little control over what the program will output given previously unseen input data. In future sections, I describe how structuring a DNN and regularization can restore some amount of control. 

\subsection{Interpretability}
\label{sec:interpretability}
When weights learned by the program can be explained---connected to understandable features such as fundamental frequency or spectrogram amplitude---a program is interpretable. A linear model is one example of such a program. A ``black box'' program, where the weights take on an abstract meaning, is difficult to interpret. 

Interpretability is often desirable when developing machine or deep learning models, as described by Molnar \cite{molnar2020interpretable}. Interpretability makes it easier to understand why the program fails under certain conditions. This leads to the ability to debug and to build safety measures. Interpretability also can be used to ensure only relevant features are used for predictions, building reliability addressing problems such as bias against minorities. Furthermore, it can build trust among users, especially when stakes are high, e.g., in the medical field. Finally, it makes it possible to ask Why instead of What, leading towards development of new theories. 

Interpretability is not as crucial when the stakes are low or when the problem is well studied. One example of such a program is optical character recognition, where there is enough practical experience with the model and problems have been addressed over time. Furthermore, even in the case of models that are not directly interpretable, other techniques are being developed to methodically detect and remove bias e.g., \cite{jiang2019identifying}.

\subsection{Expressivity}
A third concept useful for characterizing a machine or deep learning program for music technology is its expressivity. The model's structural properties determine which functions it is able to compute. In the case of DNNs, complexity of the computed function has been shown to grow exponentially with depth \cite{raghu2017expressive}. More generally, higher expressivity can come at the expense of some control and interpretability. Wager describes how machine learning can be used to avoid extraneous modeling assumptions in causal inference. \cite{wager2019causal} Approaches using easily interpretable models (e.g., linear regression), often make strong but powerful assumptions. Some assumptions, though, are neither scientifically nor methodologically motivated. A partially linear model is more expressive and enables addressing more complex situations such as treatment heterogeneity, which would have been outside the family of functions expressible by the linear model. Its parameters can be estimated using generic machine learning tools. 

Though more expressive models are not always interpretable, increasing interpretability is an active research area. The boundary between understandable and ``black box'' models is not fixed. Examples include techniques for examining what deep convolutional neural networks learn e.g., by visualizing inputs that maximize the activation of the filters \cite{qin2018convolutional}; restructuring of convolutional neural networks as probabilistic models \cite{patel2016probabilistic}; and research on interpretable parameters in reinforcement learning \cite{verma2018programmatically}. 

\subsection{Where the proposed approach stands}
The proposed deep neural network autotuning provides a relatively low amount of control or interpretability, but is much more expressive than existing approaches. 

In \ref{sec:pretheory}, I briefly argue that a high level of control over exactly what should happen musically under a comprehensive list of circumstances is not necessary for music making. It might actually be misaligned with how even top musicians perform. Though a top musician has technical mastery of his or her musical instrument, much randomness can occur during a performance, and the musician is reacting spontaneously to other musicians' actions and other factors such as the mood they are in. I assume that such reactions are not fully controlled by the musician. Furthermore, looking back, the musician is likely not able to comprehensively interpret which factors led to specific successful or unsuccessful artistic decisions. 

As I describe in more detail throughout the thesis, the proposed approach is more expressive than Auto-Tune. Reasons include the fact that it can predict pitch shifts to any frequency on the continuous scale instead of to a discrete value; that it can be trained to apply to musical genres and cultures that do not use the equal-tempered scale; that it utilizes vocals and backing track timbre instead of measured vocals fundamental frequency to make predictions. The proposed model is by far not the most expressive one could develop. For example, it predicts a constant shift for every note instead of a continuous, frame-by-frame shift. This design decision increases control over the output audio by severely restricting how it can be modified. 

\subsubsection{What happens when the program fails?}
As described in \ref{sec:interpretability}, it is important to consider what types of erroneous output or bias the program might produce, consider how to prevent it, and determine how much to prioritize minimizing these wrong outputs compared to expressivity. Example where avoiding wrong outputs is the top priority is the design of self-driving cars, where a physical risk is involved, or a credit score predictor, where bias can impact an individual's economic status. 

The effect of an automatic pitch correction depends on the context in which it is used. The context for the proposed program is as a post-processing tool for an amateur, karaoke-style performance. If the user applies the plug-in, and a note gets worse, I imagine two negative outcomes. One is that the user notice the mistake and decide not to use the plug-in, even if it might improve other notes. Another is that a user with low confidence in his or her ability to hear pitch would assume the program is correct, and as a result feel less confident or accept a bad result. However, a solution to both negative outcomes would be to design the program to be interactive, and make clear to the user that, as a machine-learning model, the autotuner sometimes makes mistakes. The interaction would, for example, let the user listen to ``before'' and ``after'' segments and decide note by note whether or not to accept the result. Optionally, the user could even adjust the prediction. Framed in this way, I imagine that the program might actually serve as an ear training tool, which lets singers refine their ability to hear subtle pitch shifts and how these affect intonation. With excellent design, detecting mistakes might actually make the program more fun to interact with and musically stimulating than a program that enforces its model of in-tune singing on the user's performance and ``considers'' itself to always be correct. Ideally, the program could even learn from user feedback to avoid repeating mistakes and become tailored to individual preferences and styles.

As I describe in more detail in later chapters, bias can an issue for this program, especially in the context of less common musical genres. Training a model on pop will likely make it unuseable for Classical Indian music or blues because the singing styles are very different. Just as singers develop a style within a specific musical style, and build on a specific musical culture, the model should be trained on the appropriate dataset. It is also important to include a variety of vocal timbres and even accents as vowel formant affects timbre. 

What becomes clear is that, in deploying the proposed autotuner, it would be important to involve professional designers and/or music theorists or musicologists to ensure a positive user experience and to make sure that musical genres and singing styles are properly represented.

\section{todo}

We can insert the development of an autotuning system into the intervention-based research (IBR framework discussed by
Adjust our assumption
Reassess technical capacity, data science
What we know about pitch in singing
Elements not included
Rich history
Western World scope
What would a more musically refined autotuner consist of?
What is our goal?

Currently shown to produce good results on ill-defined tasks. SOA. 
In the field of audio processing, deep learning has proven to be a technology that lets us move beyond MIDI, representing audio at the level of the sample or spectrogram bin. The software and hardware developed for deep learning tasks are powerful enough to process audio data in its full complexity and preserve its richness of audio.
Not currently explainable, but active research (cite).
Thesis is a small piece of this puzzle.
Avoid strong assumptions \cite{wager2019causal}.
Risk assessment: failure, discrimination.
Population demographics give no reason not to have a balanced dataset. Need to address genre.
practical use, how to address mistakes.
Even if accuracy is not perfect, can gain insights by analyzing the predictions of the model. Can be used for ear training. The user can correct the few off-pitches.

Simplified model. Define objective accuracy measure. the singer is on or close to the correct pitch according to an objective measure. This can be a simple model (proximity to equal-tempered scale as defined in a score) or something more complex (theories of musical intonation, e.g., just intonation, etc...). These models can be complex, culture-specific, and sometimes conflicting (e.g, Pythagorean versus Just). Also, not all requirements can be perfectly met (book on renaissance musical theory). Still very complex, prone to exceptions. 

Data-driven approach. Similarity to good performances of the same genre. We choose to use this approach.

work with designers and music theorists
