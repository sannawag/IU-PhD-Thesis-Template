\chapter{Towards data-driven automatic pitch correction}
\label{chap:tech-background}
The previous chapter provided a musical background for designing an automatic pitch correction system based on an empirical perspective on musical intonation. This chapter addresses the practical and technical aspects of implementing such a system. It focuses on the current state of music and audio representation in software, and on the state of data science with respect to complex problems such as music. One choice that needs to be made is how to represent music and audio data in the program. Should the program process thousands of audio samples per second, or abstractions such as notes? This chapter provides an overview of recent advancements in computation power and machine learning that make possible rich music representations. A second choice that needs to be made is whether to adopt a model-driven or a data-driven approach. This chapter compares the two and hybrids that lie in-between. When using a model, one requires thorough understanding of the task, or abstraction and simplification of the task until it becomes manageable. When using a data-driven approach, one relies on data to automatically learn meaningful patterns, but often needs to work with less understanding. This chapter discusses successful examples of both types, showing how they contribute to greater understanding in audio and related fields. It also considers what a model-driven versus a data-driven approach to automatic pitch correction would look like. It relates Pythagorean and Just ratio-based conceptualizations of musical intonation to a model-driven approach, and argues that Antares Auto-Tune is model-driven. It then considers how difficult it would be to formulate the empirically derived conceptualization as a model. This discussion belongs to the wider theme of how much one should prioritize controllability, expressivity, and interpretability in a given program. Tradeoffs between the three are often required. This chapter provides examples where, model-driven approaches tend to provide more control and interpretability, but data-driven approaches are more expressive when applied to complex tasks. This chapter considers how important these features are in the case of automatic pitch correction and what type of program might be appropriate for the task. It provides grounds for the choice in this thesis to use a data-driven approach---specifically, a deep-learning program. 

\section{Music representation in software}
Technical writer Jaron Lanier wrote in 2010 about the invention of the \gls{midi}: \begin{quote}One day in the early 1980s, a music synthesizer designer named Dave Smith casually made up a way to represent musical notes. It was called \gls{midi}. His approach conceived of music from a keyboard player's point of view. \gls{midi} was made of digital patterns that represented keyboard events like ``key-down'' and ``key-up''. 

That meant it could not describe the curvy, transient expressions a singer or a saxophone player can produce. It could only describe the tile mosaic world of the keyboardist, not the watercolor world of the violin. But there was no reason for \gls{midi} to be concerned with the whole of musical expression, since Dave only wanted to connect some synthesizers together so that you would have a larger palette of sounds while playing a single keyboard. In spite of its limitations, \gls{midi} became the standard scheme to represent music in software. Music programs and synthesizers were designed to work with it, and it quickly proved impractical to change or dispose of all the software and hardware. \gls{midi} became entrenched, and despite Herculean efforts to reform its on many occasions by a multi-decade-long parade of powerful international commercial, academic and professional organizations, it remains so. \cite[p.~7]{lanier2010you}\end{quote}

A decade later, the state of music and audio technology looks somewhat different. \gls{midi} is still a very common tool. However, audio technologies such as phase vocoding and deep learning have become available alternatives. Section \ref{sec:control-interpret-express} provides examples of programs that use them. These technologies make it possible to process audio at the level of the spectrogram frame or sample, preserving as many nuances as desired. In later sections, I provide examples of such technologies. Discretization of musical events in \gls{midi} can be compared to the discretization of musical intervals in the ratio conceptualization of musical intonation discussed in Section \ref{sec:ratio}. Both provide a simple and elegant framework for structuring music. Both discard nuances. It is up to the programmer's or designer's discretion to choose which type of representation to use. As described in more detail in later sections and chapters, the automatic pitch correction system proposed in this thesis combines some \gls{midi}-like discretization with a continuous-valued pitch shift representation, combining some of the advantages---and disadvantages---of both.

\section{Model-driven approaches}
A common debate in multiple research areas including audio and music is whether to use a model-driven or a data-driven approach for making predictions. This section explores the model-driven approach and discusses its advantages and limitations. 

When developing a model of musical intonation, one needs to explicitly define a set of variables that affect how in tune an interval sounds. One also needs to define the function that maps the variables' values to pitch correction predictions. One needs to understand the process well enough to organize the variables and functions, or simplify the process to a level of abstraction where one is able to organize it. The assumptions one makes and the variables one choose to include in the model can stem from one's place in space and time and to values one holds about the world \cite{bacharach1989organizational}. In fact, ``multiple models for the same target system do not generally stand in a deductive relationship, as they often contradict each other.'' \cite{sep-models-science} 

One might ask, is a model worth using even though it does not capture the full complexity of the real world? The Stanford Encyclopedia of Philosophy entry on models in science describes how \begin{quote}models are vehicles for learning about the world. Significant parts of scientific investigation are carried out on models rather than on reality itself because by studying a model we can discover features of, and ascertain facts about, the system the model stands for. [...] Learning about a model happens in two places: in the construction of the model and in its manipulation (Morgan 1999). There are no fixed rules or recipes for model building and so the very activity of figuring out what fits together, and how, affords an opportunity to learn about the model. Once the model is built, we do not learn about its properties by looking at it; we have to use and manipulate the model in order to elicit its secrets. \cite{sep-models-science}\end{quote} Furthermore, the entry describes how models typically provide a distorted or idealized representation of their targets, or at least one that is only approximately true. Elgin argues that learning occurs not despite, but because, of models being false \cite{elgin2017true}. 

The Encyclopedia entry provides reasons for why models help us organize our knowledge of the world and confront it with reality. Given this capability, models are useful for building more sophisticated understanding over time, and can even lead to theory development \cite{sep-models-science}.

Pythagorean and Just conceptualizations of musical intervals as ratios can be formulated as a model: Good intonation is produced between two notes---when they are played consecutively or simultaneously and both are audible---by shifting their fundamental frequencies so that their ratio is one of a set of specified values: 1:1, 2:1, 3:2, etc...because these ratios minimize shared period length, eliminating roughness and beats. This model is very elegant and provides a high level of abstraction. For example, it applies to all musical contexts and all timbres. Furthermore, it can be confronted with reality via empirical studies of musical intonation, leading to scientific knowledge. Finally, this model has provided the basis for much learning---e.g., development of sophisticated Classical Western tonality---despite the fact that it does not correspond to results of empirical studies. 

\subsection{Auto-Tune as a model}
Section \ref{sec:interval-conceptualization-design}, which describes in detail the assumptions behind Auto-Tune, argues that Auto-Tune's design is based on the ratio conceptualization of musical intervals. This means that it is a model-driven program. In practice, implementing the ratio model is often mathematically impossible, as described in section \ref{sec:ratio}. To fix this issue, Auto-Tune moves from Pythagorean and Just intonation to the equal-tempered scale, a the compromise that makes the model usable in practice, while sacrificing some of the purity of the intervals.

\subsection{The proposed system as a model}
\label{sec:proposed-as-model}
When building a model based on psychoacoustics and on results from empirical studies of intonation, the number of variables increases---or explodes. This section lists some possible features that might be relevant to determining whether a note is likely to sound in tune as judged by a listener. The list is not comprehensive, but provides a general idea.

The first set of features relates to the music. It includes the genre of the piece being performed (including stylistic choices common for the genre, such as pitch bending), backing track instrumentation (number of instruments, timbre, relative amplitude), global and local tempo, duration of the given note, harmonic and melodic context.

The second set of features relates to the singer. It includes vocal type (soprano, alto, tenor, bass), vocal characteristics (timbre, breathiness) fundamental frequency characteristics (vibrato rate, vibrato depth, and jitter \cite{devaney2020new}), vocal training (years of training, genre e.g., classical, jazz, rock), general musical training (not related to singing), vocal ability (range, stability, expressiveness), personal aesthetics, amount of vibrato, ADSR envelope of the given note, musical choices and successful and undesired outcomes during the note and earlier in the performance.

The third set of features relates to the listener. It includes musical background (musical genre familiarity and affinity, culture, musical training, hearing ability, subjective preference), audio quality (live, analog, digital, compression, sample rate, bit depth, speaker quality, number of channels), room acoustics (reverberance, distance from the source to the listener), perceived intonation accuracy of previous notes, relationship of listener to performer (self, teacher, parent, fan, audience member who bought an expensive ticket).

More generally, the theory should include aspects of what is known about the the physics of sound and psychoacoustics, cochlea structure, masking, etc...

Given all of these features, many of which are hard to quantify or measure objectively, and their complex interactions, a model that encompasses the psycho-cultural aspect of musical intonation is challenging to develop. This provides one reason for considering a data-driven approach.

\section{Data-driven approaches}
\label{sec:pretheory}
Taking a data-driven approach makes it possible to incorporate a large number of variables without fully understanding how they interact, and therefore to build a program that is better aligned with the real world than what we are explicitly able to explain. Section \ref{sec:control-interpret-express} discusses this tradeoff between understanding and expressivity. Moving from a model-driven to a data-driven approach has often ultimately led to improvement in modeling, and vice versa. An example of the former is the SIFT feature representation from computer vision, which inspired an encoder-decoder based feature extractor with a similar structure to SIFT, but learned abstract parameters, which produced higher accuracy \cite{zheng2017sift}. An example of the latter, from natural language processing, is the invention of a linguistically motivated, count-based approach to word vector embedding \cite{pennington2014glove}, which was inspired by \texttt{word2vec}, a deep learning model that had outperformed previous model-driven approaches \cite{mikolov2013efficient}. These examples illustrate how a data-driven approach can help move beyond arbitrary decisions such as SIFT feature extraction parameters, and can also demonstrate through its high accuracy how well a model has the potential to perform if formulated properly, inspiring the design of better models.

This toggling between model-driven and data-driven approaches relates to how we interact with music, both in its acoustic and digital forms. Music is not considered something that we, as humans, understand deeply or can model, despite the mature field of music theory and the existence of multiple sub-models and sub-theories that address various aspects of how music works. This has not stopped humanity from developing musical instruments and making music. Development of models of harmony has lead to innovation by composers, and when composers have broken ``the rules''---for example, when European Medieval singers added less pure intervals to their musical vocabulary of octaves and fifths---this has led to new models. Continuing this tradition in music technology might lead to new means of musical expression, new developments in music theory, or both of the above.

\section{Developing music technology: Control, interpretability, and expressivity}
\label{sec:control-interpret-express}
When developing music technology, one needs to make choices regarding how much control the programmer has over the output, how interpretable the program's actions are, and how complex functions the program can express. These features tend to come at the expense of one another. This section describes the three features and considers which ones should be prioritized for the proposed automatic pitch correction system. 

\subsection{Control}
Programmers usually provide a computer with exact, step-by-step instructions. The computer then executes these exactly, with no ability to deal with ambiguity. When designing music technology, a natural, naive approach is to provide the computer precise instructions for what to output under a comprehensive list of circumstances. This approach provides exact control over the output. However, formulating this set of instructions is developing a fully deterministic model. As described in section \ref{sec:ratio}, this tends to result in simplifications and loss of richness. One can note that Auto-Tune is one example of a program that provides full control over the output.

Developments in machine learning and deep learning have provided a framework for inserting randomness into music technology---both into model and data-driven approaches. Specifically, they utilize computers' capacity to generate random numbers and to follow statistical distributions. They also provide a framework for computers to detect or learn patterns in data. Incorporating randomness into music technology removes some control. Music Plus One \cite{raphael2010music}, a musical accompaniment system, provides an example. The accompaniment track dynamically adjusts tempo based on a soloist's actions. The adjustments are based on real-time score matching using a hidden Markov model and future note timing prediction based on a Kalman filter-like model. Though the structure of these models is precisely determined by the programmer, and therefore controlled, the actual values adopted during a performance are out of the programmer's control. The result is much more dynamic than an alternative, where the programmer would have enforced specific tempi under specific circumstances. 

Other machine learning programs provide less control than Music Plus One. \gls{nmf} \cite{LeeDD2000nips}---used, e.g., for magnitude spectrogram processing---is one such example. A \gls{nmf} program represents a matrix as a set of basis vectors and activations. It iteratively minimizes the error between the input and reconstruction. The outcome is partially under the programmer's control, as the program enforces desired characteristics such as non-negativity. However, the model can converge to multiple different results, and this aspect cannot be pre-determined. A \gls{dnn} is by default even less controllable. The programmer can design the input and target data to the program and train the model to output data that is similar to the target, but has little control over the weights that the model learns. The programmer also has little control over what the program will output given previously unseen input data. Using a \gls{dnn}, however, does not necessarily result in full loss of control. Examples in later sections illustrate how a programmer can use knowledge about a task to structure and regularize a \gls{dnn} in a way that restores some amount of control. 

\subsection{Interpretability}
\label{sec:interpretability}
When weights learned by the program can be explained---connected to understandable features such as fundamental frequency or spectrogram amplitude---a program is interpretable. A linear model is one example of such a program. A ``black box'' program, where the weights take on an abstract meaning, is difficult to interpret. 

Interpretability is often desirable when developing machine or deep learning models, as described by Molnar \cite{molnar2020interpretable}. Interpretability makes it easier to understand why the program fails under certain conditions. This leads to the ability to debug and to build safety measures. Interpretability also can be used to ensure only relevant features are used for predictions, building reliability and addressing problems such as bias against minority populations. Furthermore, it can build trust among users, especially when stakes are high, e.g., in the medical field. Finally, it makes it possible to ask \textit{Why} instead of \textit{What}, leading towards development of new theories. 

Interpretability is not as crucial when the stakes are low or when the problem is well studied. One example of such a problem is optical character recognition, where there is enough practical experience with the model and shortcomings have been addressed over time. Furthermore, even in the case of models that are not directly interpretable, other techniques are being developed to methodically detect and remove bias e.g., \cite{jiang2019identifying}.

\subsection{Expressivity}
A third concept useful for characterizing a program for music technology is its expressivity. The model's structural properties determine which functions it is able to compute. Higher expressivity can come at the expense of some control and interpretability. Wager provides an example in the context of causal inference, describing how machine learning can be used to avoid extraneous modeling assumptions. Approaches using easily interpretable models (e.g., linear regression), often make strong but powerful assumptions. Some assumptions, though, are neither scientifically nor methodologically motivated. A partially linear model is more expressive and enables addressing more complex situations such as treatment heterogeneity, which would have been outside the family of functions expressible by the linear model. Its parameters can be estimated using generic machine learning tools. \cite{wager2019causal} 

In the case of \gls{dnn}s, complexity of the computed function has been shown to grow exponentially with depth \cite{raghu2017expressive}. Even a single-layer network can theoretically express any function as long as it contains enough weights. \gls{dnn}s are often highly expressive but notoriously difficult to control or interpret. They are sometimes referred to as \textit{black boxes}, when the processing that occurs between the input and the output is not understood. \textit{Conv-TasNet} is one example of such a network \cite{luo2019conv}. It is designed for source separation and takes as input the time-domain audio signal without any pre-determined feature extraction such as a \gls{stft}. A time-domain signal is harder to interpret than a time-frequency signal, where separation of the signal into its frequency  components corresponds to the way the human auditory system processes sound. Trained people might even decipher music or speech from a spectrogram \cite[][Ch.~3, p.~41]{raphael-i547}. In the time domain, many audio signals that look quite different can sound the same to the listener, because the only difference is the phase offset \cite{engel2020ddsp}. \textit{Conv-TasNet}, instead of a \gls{stft}, uses multiple convolutional filtering layers that are trained to produce an optimal feature extraction for the task of source separation. The large number of filtering layers are hard to interpret, especially because of non-linear activation functions applied to the outputs, such as parametric rectified linear units---a ramp function where the negative side is modified to have small nonzero weights \cite{he2015delving}. After these layers of filtering, the signal is processed in an unknown embedding space, which is again useful for the task but hard to interpret.

Though more expressive models are not always fully interpretable, increasing interpretability is an active research area. The boundary between understandable and \textit{black box} models is not fixed. Examples of techniques that increase understanding involve examination of what deep convolutional neural networks learn by visualizing inputs that maximize the activation of the filters \cite{qin2018convolutional}; restructuring of convolutional neural networks as probabilistic models \cite{patel2016probabilistic}; and research on interpretable parameters in reinforcement learning \cite{verma2018programmatically}. 

\subsection{Priorities in the proposed system}
How important is it for the proposed system to be controllable, interpretable, and expressive?
%The proposed deep neural network autotuning, which I describe in detail in later chapters, provides a relatively low amount of control or interpretability, but is much more expressive than existing approaches.

Section \ref{sec:pretheory} briefly argues that a high level of control over exactly what should happen musically under a comprehensive list of circumstances is not necessary for music making. That might actually be misaligned with how even top musicians perform. Though a top musician has technical mastery of their musical instrument, much randomness can occur during a performance, and the musician is reacting spontaneously to other musicians' actions and other factors such as the mood they are in. Such reactions are not fully controlled by the musician. Furthermore, thinking back to a performance, the musician is likely not able to comprehensively interpret which factors led to specific successful or unsuccessful artistic decisions. 

Expressivity, on the other hand, is what the proposed approach strives to offer more than Auto-Tune. The proposed system strives to predict pitch shifts in the context of physics of sound, psychoacoustics, and musical cultures. Section \ref{sec:proposed-as-model} describes the large number of moving parts in the empirical approach. Given empirical studies of intervals in musical performances, it should predict pitch corrections on a continuous scale instead of mapping frequency to a discrete set of values. It should apply to musical genres and cultures that do not use the equal-tempered scale. Furthermore, given that it builds on psychoacoustics, it strives to utilize the full vocals and backing track timbres to make predictions instead the fundamental frequency extracted from the vocals. These goals result in an automatic pitch correction system that takes more complex data as input and outputs more complex predictions. It likely requires a very expressive system.

The requirements for expressiveness, however, to not require removing all control and interpretability. The proposed system can still use a time-frequency transformation as a pre-processing step instead of directly process the time-domain audio signal. It can also use \gls{midi}-like abstractions of notes, processing notes instead of audio samples. As described in detail in later chapters, the proposed model applies a constant transformation to every note, restricting its outputs compared to a model that could transform every sample. This design decision prevents undesirable artifacts from occurring during a note. It increases control, and decreases expressiveness. Later chapters also compare this approach to an end-to-end model, where both the input and output are time-domain signals. 

\subsection{What happens when the pitch correction fails?}
When working with a model that is not fully controllable or interpretable, it is important to consider what types of erroneous output or bias the program might produce, consider how to prevent it, and determine how much to prioritize minimizing these wrong outputs compared to expressivity. An example where avoiding wrong outputs is the top priority is the design of self-driving cars, where a physical risk is involved, or a credit score predictor, where bias can impact an individual's economic status. 

The effect of an automatic pitch correction error depends on the context in which the program is used. The context for the proposed program is as a post-processing tool for an amateur, karaoke-style performance. If the user applies the plug-in, and a note gets worse, one can imagine two negative outcomes. One is that the user notices the mistake and decides not to use the plug-in, even if it might improve other notes. Another is that a user with low confidence in their ability to hear pitch assumes the program is correct, and as a result feels less confident or accepts a bad result. However, a solution to both negative outcomes would be to design the program to be interactive, and to make clear to the user that, as a machine-learning model, the system sometimes makes mistakes. The interaction would, for example, let the user listen to ``before'' and ``after'' segments and decide note by note whether or not to accept the result. Optionally, the user could even adjust the prediction. Framed in this way, the program might actually serve as an ear training tool, which lets singers refine their ability to hear subtle pitch shifts and how these affect intonation. With excellent design, detecting mistakes might actually make the program more fun to interact with and musically stimulating than a program that enforces its model of in-tune singing on the user's performance and ``considers'' itself to always be correct. Ideally, the program could even learn from user feedback to avoid repeating mistakes and become tailored to individual preferences and styles.

As described in later chapters, bias can an issue for this program, especially in the context of less common musical genres. Training a model on pop will likely make it unusable for Classical Indian music or blues because the singing styles are very different. Just as singers develop a style within a specific musical style, and build on a specific musical culture, the model should be trained on the appropriate dataset. It is also important to include a variety of vocal timbres and even accents because vowel formant affects timbre. 

What becomes clear is that, in deploying the proposed automatic pitch correction system, it is important to involve professional designers and/or music theorists or musicologists to ensure a positive user experience and to make sure that musical genres and singing styles are properly represented.

\subsection{The usefulness of deep learning for automatic pitch correction}
In the field of audio processing, deep learning has proven to be a technology that lets us represent audio in a richer manner than abstract notes---as in \gls{midi}---processing audio at the level of the sample or spectrogram bin. The software and hardware developed for deep learning tasks are powerful enough to process audio data in its full complexity. Deep learning methods have been shown to produce impressive, and state-of-the-art results on ill-defined audio tasks such as generating natural-sounding speech for home assistants \cite{oord2016wavenet}, pitch detection \cite{bittner2017deep} or dereverberation \cite{su2020hifi}. The first example, \textit{Wavenet} is a deep generative model of raw audio waveforms. It was shown to reduce the gap with human performance by over 50\% compared to the state of the art in Text-to-Speech systems. The model is fully probabilistic and autoregressive. Oord \textit{et al.} write that it is able to produce natural sounding speech based on its training on tens of thousands of samples of audio per second. The \gls{dnn} structure, in this case, provides the ability to work efficiently at the level of the audio sample on speech, which is a complex signal. The second example uses a deep convolutional network to extract features from a time-frequency representation of polyphonic audio. Extracting pitch information when there are multiple instruments is challenging, but the large number of filtering layers provides the model the ability to extract useful information from a complex signal. The third example is based on \textit{Wavenet}, takes as input reverberant audio, and outputs a dryer version of the signal. In this case, the \gls{dnn} provides the ability to work with raw audio, not discarding much of the richness of the signal despite removing room reflections.

The automatic pitch correction task has similar goals to the above examples. It aims to extract frequency and interval information from vocals and backing tracks, and to process complex audio and music data. The success of \gls{dnn}s in audio provides a reason for considering using on for the given task.