\chapter{Designing and implementing music technology}
\label{chap:tech-background}
Chapter \ref{chap:intonation} discusses the music theoretical background to designing an autotuner, focusing on musical intonation. It introduces Antares Auto-Tune's functionality and relates it to a ratio-based conceptualization of musical intonation. It argues for adopting an empirically based approach to musical intonation. This chapter focuses on the technical aspect of building an autotuning system. Ultimately, the quality of the proposed autotuner doesn't only depend on the sophistication of the assumptions about musical intonation we include in out design: It also depends on how well we can implement our approach as a computer program. This question can reach different conclusions every year, given Moore's Law that computation power is rapidly increasing, and given fast developments in data science. This chapter first explores music and audio representation capability in software. It then compares a model-driven approach to autotuning to a data-driven approach. It provides examples related to music and audio processing programs that fall into the two categories, exploring how their design affects the degree to which their outcome can be controlled and interpreted, and how complex functions they can express. It then discusses the concepts of control, interpretability, and expressiveness and how much these should be prioritized in the proposed autotuner. 

\section{Music representation in software}
Technical writer Jaron Lanier wrote in 2010 about the invention of the \textit{Musical Instrument Digital Interface}, or \textit{MIDI}: ``One day in the early 1980s, a music synthesizer designer named Dave Smith casually made up a way to represent musical notes. It was called MIDI. His approach conceived of music from a keyboard player's point of view. MIDI was made of digital patterns that represented keyboard events like ``key-down'' and ``key-up''. 

That meant it could not describe the curvy, transient expressions a singer or a saxophone player can produce. It could only describe the tile mosaic world of the keyboardist, not the watercolor world of the violin. But there was no reason for MIDI to be concerned with the whole of musical expression, since Dave only wanted to connect some synthesizers together so that you would have a larger palette of sounds while playing a single keyboard. In spite of its limitations, MIDI became the standard scheme to represent music in software. Music programs and synthesizers were designed to work with it, and it quickly proved impractical to change or dispose of all the software and hardware. MIDI became entrenched, and despite Herculean efforts to reform its on many occasions by a multi-decade-long parade of powerful international commercial, academic and professional organizations, it remains so.'' \cite[p.~7]{lanier2010you}

A decade later, the state of music and audio technology does not look quite as bleak. MIDI is still a very common tool. However, refined audio technologies such as phase vocoding and deep learning have become available alternatives. These technologies make it possible to modify audio at the level of the spectrogram frame or sample. In later sections, I provide examples of such technologies. Discretization of musical events in MIDI can be compared to the discretization of musical intervals using ratio conceptualization discussed in \ref{sec:ratio}. Both provide a simple and elegant framework for structuring music. Both discard nuances. It is up to the programmer's or designer's discretion to choose which types of representations to use. The autotuner proposed in this thesis combines some MIDI-like discretization with a continuous-valued pitch shift representation, combining some of the advantages (and disadvantages) of both.

\section{A model-driven approach to autotuning}
A common debate in multiple research areas including audio and music is whether to use a model-driven or a data-driven approach for making predictions. In this section, I explore the former and discuss its advantages and limitations. I refer to chapter \ref{chap:intonation} to argue that the Pythagorean and Just ratio-based conceptualizations provide the basis for a model-driven approach, and argue that Antares Auto-Tune is model-driven. I then explore what a psycho-cultural model of musical intonation \cite{parncutt2018psychocultural} would look like when defined precisely enough to be implemented as a computer program, and why developing such a model is difficult. 

When developing a model of musical intonation, one needs to explicitly define a set of variables that affect how in tune an interval sounds. One also needs to define the function that maps the variables's values to pitch correction predictions. One needs to understand the process well enough to organize the variables and functions, or simplify the process until one is able to understand it. The assumptions one makes and the variables one choose to include in the model can be bound to one's place in space and time and to values one holds about the world \cite{bacharach1989organizational}. In fact, ``multiple models for the same target system do not generally stand in a deductive relationship, as they often contradict each other.'' \cite{sep-models-science} One can ask, is a model worth using even though it does not capture the full complexity of the real world? 

The Stanford Encyclopedia of Philosophy entry on models in science describes how ``models are vehicles for learning about the world. Significant parts of scientific investigation are carried out on models rather than on reality itself because by studying a model we can discover features of, and ascertain facts about, the system the model stands for. [...] Learning about a model happens in two places: in the construction of the model and in its manipulation (Morgan 1999). There are no fixed rules or recipes for model building and so the very activity of figuring out what fits together, and how, affords an opportunity to learn about the model. Once the model is built, we do not learn about its properties by looking at it; we have to use and manipulate the model in order to elicit its secrets.'' \cite{sep-models-science} Furthermore, the entry describes how models typically provide a distorted or idealized representation of their targets, or at least one that is only approximately true. Elgin argues that learning occurs not despite, but because, of models being false \cite{elgin2017true}. 

The Encyclopedia entry provides reasons for why models help us organize our knowledge of the world and confront it with reality. Given this capability, models are useful for building more sophisticated understanding over time, and can even lead to theory development \cite{sep-models-science}.

Pythagorean and Just conceptualizations of musical intervals as ratios can be formulated as a model: Good intonation is produced between two notes---when they are played consecutively or simultaneously and both are audible---by shifting their fundamental frequencies so that their ratio is one of a set of specified values: 1:1, 2:1, 3:2, etc...because these ratios minimize shared period length, eliminating roughness and beats. This model is very elegant and provides a high level of abstraction. For example, it applies to all musical contexts and all timbres. Furthermore, it can be confronted with reality via  empirical studies of musical intonation and the mathematical. Finally, much learning---e.g., development of Western tonal harmony---has occurred through this model despite the fact that it does not correspond to empirical studies. 

\subsection{Auto-Tune as a model}
In section \ref{sec:interval-conceptualization-design}, which describes in detail the assumptions behind Auto-Tune, I argue that Auto-Tune's design is based on the ratio conceptualization of musical intervals. This means that it is a model-based program. In practice, implementing the ratio model is often mathematically impossible, as described in section \ref{sec:ratio}. To fix this issue, Auto-Tune moves from Pythagorean and Just intonation to the equal-tempered scale, a the compromise that makes the model usable in practice, while sacrificing some of the purity of the intervals.

\subsection{The proposed approach as a model}
I now consider what a model would look like if basing it on psychoacoustics and on results from empirical studies of intonation. The number of variables increases---or explodes. I list some possible features that might be relevant to determining whether a note is likely to sound in tune as judged by a listener. The list is not comprehensive, but provides a general idea.

The first set of features relates to the music. It includes the genre of the piece being performed (including stylistic choices common for the genre, such as pitch bending), backing track instrumentation (number of instruments, timbre, relative amplitude), global and local tempo, duration of the given note, harmonic and melodic context.

The second set of features relates to the singer. It includes vocal type (soprano, alto, tenor, bass), vocal characteristics (timbre, breathiness) fundamental frequency characteristics (vibrato rate, vibrato depth, and jitter \cite{devaney2020new}), vocal training (years of training, genre e.g., classical, jazz, rock), general musical training (not related to singing), vocal ability (range, stability, expressiveness), personal aesthetics, amount of vibrato, ADSR envelope of the given note, musical choices and successful and undesired outcomes during the note and earlier in the performance.

The third set of features relates to the listener. It includes musical background (musical genre familiarity and affinity, culture, musical training, hearing ability, subjective preference), audio quality (live, analog, digital, compression, sample rate, bit depth, speaker quality, number of channels), room acoustics (reverberance, distance from the source to the listener), perceived intonation accuracy of previous notes, relationship of listener to performer (self, teacher, parent, fan, audience member who bought an expensive ticket).

More generally, the theory should include aspects of what is known about the the physics of sound and psychoacoustics, cochlea structure, masking, etc...

Given all of these features, many of which are hard to quantify or measure objectively, and their complex interactions, a model that encompasses the psycho-cultural aspect of musical intonation is challenging to develop. However, in later sections, I argue that much work towards developing an autotuner can be done without developing a model.

\section{A data-driven approach to autotuning}
\label{sec:pretheory}
The current model for autotuning is reliable but limited in its accuracy and nuance. Two choices exist. One is to try to define a better model, though this can be challenging. Another is to adopt a data-driven approach. In this section, I argue that the second choice might lead to higher advancement of knowledge. 

Taking a data-driven approach makes it possible to incorporate a large number of variables without fully understanding how they interact, and therefore to build a program that is better aligned with the real world than what we are explicitly able to explain. I discuss this tradeoff between understanding and expressivity in section \ref{sec:control-interpret-express}. Going from a model to a data-driven approach or has often ultimately led to improvement in modeling, and vice versa. An example of the former is the SIFT feature representation from computer vision, which inspired an encoder-decoder based feature extractor with a similar structure to SIFT, but learned abstract parameters, that produced higher accuracy \cite{zheng2017sift}. An example of the latter, from natural language processing, is the invention of a linguistically motivated, count-based approach to word vector embedding \cite{pennington2014glove}, which was inspired by \texttt{word2vec}, a deep learning model that had outperformed previous model-based approaches \cite{mikolov2013efficient}. More generally, these examples illustrate how a data-driven approach can help move beyond arbitrary decisions such as SIFT feature extraction parameters, and can also through its high performance show how well a model has the potential to perform if formulated properly, inspiring the design of better models.

I believe this toggling between model and data-driven approaches corresponds to how we interact with music, both in its acoustic and digital forms. Music is not considered something that we, as humans, understand deeply or can model, despite the mature field of music theory and the existence of multiple sub-models and sub-theories that address various aspects of how music works. This has not stopped humanity from developing musical instruments and making music. Development of models of harmony has lead to innovation by composers, and when composers have broken ``the rules'', this has lead to new models. Continuing this tradition in music technology might lead to new means of musical expression, new developments in music theory, or both of the above.

\section{Developing music technology: Control, interpretability, and expressivity}
\label{sec:control-interpret-express}
When developing music technology in practice, choices need to be made regarding how much control the programmer has over the output, how interpretable the program's actions are, and how complex functions the program can express. These characteristics tend to come at the expense of one another. In this section, I describe the advantages and disadvantages of increasing each of these characteristics, how they relate to model and data-driven approaches, and where I choose to place the proposed program. 

\subsection{Control}
Programmers usually provide a computer with exact, step-by-step instructions. The computer then executes these exactly, with no ability to deal with ambiguity. When designing music technology, a natural, naive approach is to provide the computer precise instructions for what to output under a comprehensive list of circumstances. This approach provides exact control over the output. However, formulating this set of instructions is developing a fully deterministic model. As described in section \ref{sec:ratio}, this tends to result in simplifications and loss of richness.

Developments in machine learning and deep learning have provided a framework for inserting randomness into music technology---both into model and data-driven approaches. Specifically, they utilize computers' capacity to generate random numbers and to follow statistical distributions. They also provide a framework for computers to detect or learn patterns in data. Incorporating randomness into music technology removes some control. Music Plus One \cite{raphael2010music}, a musical accompaniment system, provides an example. The accompaniment track dynamically adjusts tempo based on a soloist's actions. The adjustments are based on real-time score matching using a hidden Markov model and future note timing prediction based on a Kalman filter-like model. Though the structure of these models is precisely determined by the programmer, and therefore controlled, the actual values adopted during a performance are out of the programmer's control. The result is much more dynamic than an alternative, where the programmer would have enforced specific tempi under specific circumstances. I note that Auto-Tune is one example of a program that provides full control and no randomness.

Other machine learning programs provide less control than Music Plus One. Non-negative matrix factorization (NMF) \cite{LeeDD2000nips}---used, e.g., for magnitude spectrogram processing---is one such example. A NMF program represents a matrix as a set of basis vectors and activations. It iteratively minimizes the error between the input and reconstruction. The outcome is partially under the programmer's control, as the program enforces desired characteristics such as non-negativity. However, the model can converge to multiple different results, and this aspect cannot be pre-determined. A deep neural network (DNN) is by default even less controllable. The programmer can design the input and target data to the program and train the model to output data that is similar to the target, but has little control over the weights that the model learns. The programmer also has little control over what the program will output given previously unseen input data. In future sections, I describe how structuring a DNN and regularization can restore some amount of control. 

\subsection{Interpretability}
\label{sec:interpretability}
When weights learned by the program can be explained---connected to understandable features such as fundamental frequency or spectrogram amplitude---a program is interpretable. A linear model is one example of such a program. A ``black box'' program, where the weights take on an abstract meaning, is difficult to interpret. 

Interpretability is often desirable when developing machine or deep learning models, as described by Molnar \cite{molnar2020interpretable}. Interpretability makes it easier to understand why the program fails under certain conditions. This leads to the ability to debug and to build safety measures. Interpretability also can be used to ensure only relevant features are used for predictions, building reliability and addressing problems such as bias against minority populations. Furthermore, it can build trust among users, especially when stakes are high, e.g., in the medical field. Finally, it makes it possible to ask Why instead of What, leading towards development of new theories. 

Interpretability is not as crucial when the stakes are low or when the problem is well studied. One example of such a problem is optical character recognition, where there is enough practical experience with the model and shortcomings have been addressed over time. Furthermore, even in the case of models that are not directly interpretable, other techniques are being developed to methodically detect and remove bias e.g., \cite{jiang2019identifying}.

\subsection{Expressivity}
A third concept useful for characterizing a program for music technology is its expressivity. The model's structural properties determine which functions it is able to compute. In the case of DNNs, complexity of the computed function has been shown to grow exponentially with depth \cite{raghu2017expressive}. Even a single-layer model can theoretically express any function as long as it contains enough weights. Higher expressivity can come at the expense of some control and interpretability. Wager describes how machine learning can be used to avoid extraneous modeling assumptions in causal inference. Approaches using easily interpretable models (e.g., linear regression), often make strong but powerful assumptions. Some assumptions, though, are neither scientifically nor methodologically motivated. A partially linear model is more expressive and enables addressing more complex situations such as treatment heterogeneity, which would have been outside the family of functions expressible by the linear model. Its parameters can be estimated using generic machine learning tools. \cite{wager2019causal} 

Though more expressive models are not always fully interpretable, increasing interpretability is an active research area. The boundary between understandable and ``black box'' models is not fixed. Examples of tehniques that increase understanding involve examination of what deep convolutional neural networks learn by visualizing inputs that maximize the activation of the filters \cite{qin2018convolutional}; restructuring of convolutional neural networks as probabilistic models \cite{patel2016probabilistic}; and research on interpretable parameters in reinforcement learning \cite{verma2018programmatically}. 

\subsection{Where the proposed approach stands}
The proposed deep neural network autotuning, which I describe in detail in later chapters, provides a relatively low amount of control or interpretability, but is much more expressive than existing approaches. 

In section \ref{sec:pretheory}, I briefly argue that a high level of control over exactly what should happen musically under a comprehensive list of circumstances is not necessary for music making. It might actually be misaligned with how even top musicians perform. Though a top musician has technical mastery of his or her musical instrument, much randomness can occur during a performance, and the musician is reacting spontaneously to other musicians' actions and other factors such as the mood they are in. I assume that such reactions are not fully controlled by the musician. Furthermore, thinking back to a performance, the musician is likely not able to comprehensively interpret which factors led to specific successful or unsuccessful artistic decisions. 

As I describe in more detail throughout the thesis, the proposed approach is more expressive than Auto-Tune. Reasons include the fact that it can predict pitch shifts to any frequency on the continuous scale instead of to a discrete set of values; that it can be trained to apply to musical genres and cultures that do not use the equal-tempered scale; that it utilizes the full vocals and backing track timbres instead the fundamental frequency extracted from the vocals to make predictions. The proposed model is by far not the most expressive one could develop. For example, it predicts a constant shift for every note instead of a continuous, frame-by-frame shift. It is inspired by MIDI's note events, and prevents undesirable artifacts from occurring during a note. This design decision increases control over the output audio by severely restricting how it can be modified, and decreases expressiveness. 

\subsubsection{What happens when the program fails?}
It is important to consider what types of erroneous output or bias the program might produce, consider how to prevent it, and determine how much to prioritize minimizing these wrong outputs compared to expressivity. An example where avoiding wrong outputs is the top priority is the design of self-driving cars, where a physical risk is involved, or a credit score predictor, where bias can impact an individual's economic status. 

The effect of an automatic pitch correction error depends on the context in which the program is used. The context for the proposed program is as a post-processing tool for an amateur, karaoke-style performance. If the user applies the plug-in, and a note gets worse, I imagine two negative outcomes. One is that the user notices the mistake and decides not to use the plug-in, even if it might improve other notes. Another is that a user with low confidence in his or her ability to hear pitch assumes the program is correct, and as a result feels less confident or accepts a bad result. However, a solution to both negative outcomes would be to design the program to be interactive, and to make clear to the user that, as a machine-learning model, the autotuner sometimes makes mistakes. The interaction would, for example, let the user listen to ``before'' and ``after'' segments and decide note by note whether or not to accept the result. Optionally, the user could even adjust the prediction. Framed in this way, I imagine that the program might actually serve as an ear training tool, which lets singers refine their ability to hear subtle pitch shifts and how these affect intonation. With excellent design, detecting mistakes might actually make the program more fun to interact with and musically stimulating than a program that enforces its model of in-tune singing on the user's performance and ``considers'' itself to always be correct. Ideally, the program could even learn from user feedback to avoid repeating mistakes and become tailored to individual preferences and styles.

As I describe in more detail in later chapters, bias can an issue for this program, especially in the context of less common musical genres. Training a model on pop will likely make it unuseable for Classical Indian music or blues because the singing styles are very different. Just as singers develop a style within a specific musical style, and build on a specific musical culture, the model should be trained on the appropriate dataset. It is also important to include a variety of vocal timbres and even accents because vowel formant affects timbre. 

What becomes clear is that, in deploying the proposed autotuner, it is important to involve professional designers and/or music theorists or musicologists to ensure a positive user experience and to make sure that musical genres and singing styles are properly represented.

\subsection{Why I choose deep learning}
As mentioned in the introduction to this chapter, there are good reasons to move beyond MIDI for music representation. In the field of audio processing, deep learning has proven to be a technology that lets us represent audio in a richer manner at the level of the sample or spectrogram bin. The software and hardware developed for deep learning tasks are powerful enough to process audio data in its full complexity. Deep learning methods have been shown to produce impressive, and state-of-the-art results on ill-defined audio tasks such as generating natural-sounding speech for home assistants \cite{oord2016wavenet}, pitch detection \cite{bittner2017deep} or dereverberation \cite{su2020hifi}. 