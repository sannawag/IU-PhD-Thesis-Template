\chapter{Introduction}
\label{sec:thesis-intro}

\section{Post-processing of user-generated audio: An overview}
\label{sec:challenges-intro}
When people use applications such as Smule, Spotify, Cadenza, YouTube, and TikTok to create a digital audio recording and share them with friends or a wider audience, they may wish to post-process the recording to make it the best possible quality. Post-processing can address errors in musical intonation or improve room acoustics that reduce sound quality and intelligibility. Some users opt to use professional audio-editing software, often also choosing to record in a studio with high-quality acoustics. Other users treat the production of audio more recreationally. For example, a Smule user may spend a few minutes recording a karaoke performance in their room or car. Though the user may not wish to spend much time editing the recording, he or she often still uses post-processing tools built into the app that apply Auto-Tuning or add reverb. These tools do not make the recording sound professional, but produce a quite good result that the user can be pleased with and is motivated to share with friends. These post-processing tools are also useful for users who do not have access to professional software or recording spaces, or find their usage difficult because they are not trained in audio editing. 
    
A parallel can be seen when a person uses a spell-checker to improve the quality of their writing, even when writing simple text such as an email. The ability of the spell-checker to remove minor errors produces a more polished result, which can make a big difference, especially given the permanent nature of content shared online.

This thesis addresses the task of developing the equivalent of a spell-checker for audio. It focuses on two types of post-processing. The first is automatic pitch correction. Existing commercial autotuning programs usually discretize the pitch to a small number of values \cite{antares:2016}. Vocal track notes are usually shifted to be centered around pitches in a user-defined score, or mapped to the closest pitch among the twelve equal-tempered scale degrees. The resulting model of pitch does not reflect the smooth variation of pitch in continuously controlled instruments such as voice or violin, and can produce a robotic sounding result. We introduce a data-driven approach to automatic pitch correction of solo singing performances. The proposed approach predicts note-wise pitch shifts from the relationship between the frequency content of the singing and accompaniment. The proposed system treats pitch as a continuous value rather than a set of discretized notes and does not rely on a musical score, thus allowing for improvisation and harmonization in the singing performance. Our model is trained on both incorrect intonation, for which it learns a correction, and intentional pitch variation, which it learns to preserve.

The second type of post-processing is reverberation reduction. We consider a Spotify user who records a podcast at home. Room acoustics in the home commonly produce undesirable effects such as excessive coloration from early reflections and masking from late reflections \cite{faller2019modifying}. We are interested both in the single- and multi-channel cases because user inputs can be of either type. A smartphone typically has two microphones. On the other hand, a user may use an external microphone with a single channel. The single-channel case is challenging because, unlike in the multi-channel case, a reverberation reduction program cannot utilize the difference in time of arrivals of reflections across the microphones. In the recent REVERB challenge \cite{kinoshita2016summary}, only one among eleven submitted single-channel systems both reduced the perceived amount of reverberation and improved overall quality.

The task of reverberation reduction has been framed in multiple ways over the past five decades. Habets describes three trends \cite{habets2016fifty}. The first involves modeling the acoustic system and using this information to equalize or filter the reverberant signal. The second approach is similar to denoising, treating the source and reverberation signals as independent. The third directly estimates the dry signal without trying to model the room acoustics.

Each of these techniques comes with its own challenges. Techniques that involve modeling the acoustic system can be challenging to develop even when the Room Impulse Response (RIR) is known. Though the RIR function is a linear transformation, in realistic room acoustics, an exact inverse is often either unstable or acausal. Exact equalization requires very long filters. Sometimes, computing the inverse can be intractable \cite{neely1979invertibility, cecchi2018room}. Another issue is that imperfections in the RIR measurement will affect the result. Faller \cite{faller2019modifying} describe how inversion-based filtering only works in the ``sweet spot''. The quality gets worse in other positions in the room. Furthermore, changes in object positions in the room, temperature, or humidity will change the RIR. It can be described as a weakly non-stationary process \cite{cecchi2018room}. For this reason, techniques that remove only the most problematic components of the reverberation while leaving the rest intact have so far proven most practical.

Denoising approaches that work well for signals that are independent often do not generalize well to the dereverberation task: A reverberant signal is a sum of delayed and filtered versions of the original, which are highly correlated, as illustrated in Figure. Additionally, room impulse responses often last longer than one Short-Time Fourier Transform (STFT) window. For these reasons, techniques such as time-frequency bin masking \cite{hershey2016deep} may not be as suitable for dereverberation.

Direct estimation can produce successful results, but might not use all information available. For example, if the RIR is known or estimated from the audio, this could be used to further improve results. 

While current dereverberation approaches produce successful results as a pre-processing step for tasks such as automatic speech recognition, producing an artifact-free audio signal remains challenging. 

\section{Challenges in generating a natural sounding result}
The tasks of autotuning and dereverberation are similar. First, the priority with both is to output a result that sounds natural and aesthetically pleasing. If this is not the case, the user will prefer the unprocessed recording. 

Second, realistic data for both tasks is hard to generate. In the case of autotuning, the first challenge is collecting examples of in-tune singing: Publicly available datasets of singing performances typically mix performances of all levels of singing. The second challenge---if using supervised training---is to design data pairs where, for example, each pair is identical except for the vocals pitch. Such pairs are difficult to come across naturally, making realistic data synthesis a viable approach. In the case of dereverberation, exact RIR measurements are hard to record. This would require recording a Dirac delta signal in a room. Realistic signals have at least a short decay. Synthesized RIRs can be generated from a Dirac delta, but require modeling complex room acoustics and the way they change over time \cite{faller2019modifying, cecchi2018room}. Popular RIR synthesis techniques simplify the task, for example, by modeling an empty, rectangular room \cite{allen1979image}. Given the advantages and disadvantages of each, combining information from real-world and synthesized RIR recordings can prove useful.

Third, approaches to both tasks often involve error-prone or limiting data pre and post-processing steps. For example, though some dereverberation models output promising results, they often estimate only the magnitude spectrogram, then add the reverberant phase as a post-processing step. Using the reverberant phase can lead to arfitacts. Similarly, autotuning involves error-prone note boundary and pitch tracking pre-processing steps that add errors that cannot be addressed in the pitch correction component of the system. Incorporating these data processing steps into the trainable part of the system is worth investigating.

\section{Thesis overview}
This section describes an outline for the work in this thesis. Chapter describes future work in more detail. 

The first step in this thesis is to collect data to train models for autotuning and dereverberation, making the data have as few discrepancies as possible with real-world data. This involves a combination of modeling of concepts such as out-of-tune singing, hand-tuned feature design, and use of unsupervised techniques such as clustering. It can also include data synthesis using techniques such as Generative Adverserial Models (GANs) if necessary. 

The second step consists of developing simple approaches that produce moderately successful results for both tasks. Further work can build on these results. A simple approach can use lossy data representations such as the magnitude spectrogram, and somewhat unrealistic training data. 

The third step is to include data pre and post-processing steps in the main model, developing end-to-end modeling for autotuning and dereverberation. As an example, steps such as pitch tracking can either be treated as separate modules within the broader model or be fully integrated with the components that predict pitch shifts from the audio. The third step also involves iterating on the model structure to enable it to best extract information from the data. 

A fourth, optional step is only used if necessary and involves investigating applying GANs to further improve the results from supervised training and enforce that outputs sound natural.

\section{Outline}
In chapter \ref{sec:thesis-prior-work} of this thesis proposal, I describe my previous work on dereverberation and other related topics. In chapters \ref{sec:thesis-autotuner} and \ref{sec:thesis-damp}, I describe work up to now on autotuning. Chapter \ref{sec:thesis-autotuner} describes the autotuning system and current results, while chapter \ref{sec:thesis-damp} describes how I generated a dataset of singing performances to train the model. Chapter \ref{sec:thesis-dereverb} describes current work on dereverberation. Chapter \ref{sec:thesis-timeline} describes future work on autotuning and dereverberation and provides a timeline for the thesis.