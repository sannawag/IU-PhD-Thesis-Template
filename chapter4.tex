\chapter{The data-driven pitch correction algorithm}
\label{chap:thesis-autotuner}
This chapter provides details of the algorithm \cite{wager2020deep}. It begins with related work, and then moves to the neural network structure. It also covers the data preparation steps---including pitch de-tuning and feature extraction---followed by the training configuration and the experimental setup. 

\section{Open-source repository}
%- code is available
I released an implementation of the algorithm in Python using the Pytorch framework for deep learning \cite{paszke2019pytorch}. The repository is available at \url{https://github.com/sannawag/autotuner}\footnote{Note that the algorithm used to be named \textit{Deep Autotuner}, but I have modified the name when possible to avoid confusion with the trademarked term \textit{Auto-Tune} by Antares.}. Users have the option of training the model on their own dataset or of downloading the parameters of the model that provided the best test results. The repository also includes code for a baseline automatic pitch correction algorithm introduced in \ref{sec:subjective-test}, and an implementation of the \gls{psola} algorithm \cite{charpentier1986diphone}. A standalone implementation of \gls{psola} is available at \url{https://github.com/sannawag/TD-PSOLA}. The dataset used to train the model is available upon request via \url{https://ccrma.stanford.edu/damp}. Note that the \gls{cqt} parameters used in the published dataset are different from those referred to in this chapter.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{images/model_outline_1.png}
    \caption{Program overview. The program processes one note at a time, and predicts a constant shift for the note's duration. The proposed \gls{dnn} architecture includes convolutional layers for feature extraction followed by \gls{gru}s for sequential processing.}
    \label{fig:model_outline}
\end{figure}

\section{Related work}
%related work
%pitch correction
%- mention antares
The first commercial pitch-correction technique, Antares Auto-Tune \cite{antares:2016}, is also one of the most commonly used. Section \ref{sec:autotune} describes how it is designed. Auto-Tune measures the fundamental frequency of the input monophonic singing recording, then re-synthesizes the pitch-corrected audio signal. In recent work on continuous score-coded pitch correction \cite{salazar2015continuous}, as in Auto-Tune, each vocal note is pitch shifted to the nearest note in a user-input set of pitches (scale) or to the note in the score if it is known. The default musical scale is the equal-tempered scale, in which each pitch $p$ belongs to the set of \gls{midi} pitches $[0, 1, ..., 127]$ and its frequency in Hertz is defined as $440*2^{\frac{p-69}{12}}$. Some users prefer a finer resolution and include more than twelve pitches per octave, or use intervals of varying sizes between pitches. In every case, the fundamental frequency is discretized to a small set of values, around which every note is shifted to be exactly centered. Hence, the pitch shifts tend to ignore a singer's intentional expressive gestures and might not easily apply to musical traditions with different scales or more fluidly varying pitch. The proposed algorithm accommodates a variety of frequencies by letting the fundamental frequency take any value along a continuous scale, and by shifting every note by a constant without modifying internal pitch variation.

%- other pitch correction programs
Other recent approaches to pitch correction include style transfer. Style-transfer-based work modifies amateur performances to mimic a professional-level performance of the same song. Luo \textit{et al.} propose to match the pitch contour of the professional-level performance while preserving the spectral envelope of the amateur performance \cite{luo2018singing}. Meanwhile, Yong and Nam propose to match both the pitch and amplitude envelopes \cite{yong2018singing}. The approach in this thesis is similar in the sense that it also uses features gathered from high-quality performances \cite{wager2018intonation}. However, it does not necessitate a ``target" performance of the same song during testing. Instead, it learns from many in-tune singing voice examples and their backing tracks, and then generalizes to unknown songs, while preserving the original singer's style.

\subsection{Music information retrieval}
%pitch detection
%    - pitch detection (ok)
%    - \gls{cqt} (ok)
While only a few algorithms exist for pitch correction, \gls{mir} research on related tasks such as pitch detection provides a useful background for this thesis. Gomez {\it et al.} \cite{gomez2018deep} provide an overview of recent developments in deep learning for singing processing, ranging from pitch detection to singing separation and synthesis. Pitch detection is particularly relevant to automatic pitch correction. Pitch detection algorithms, like the algorithm in this thesis, aim to extract harmonic patterns from the audio. In the case of pitch detection, the target pitches are often manually labeled. 

Bittner \textit{et al.} introduce a fully convolutional \gls{dnn} for polyphonic pitch detection and transcription. The input is the magnitude \gls{hcqt} of the audio. The \gls{cqt} is a time-frequency transformation suitable for a convolutional neural network, which I also use in this thesis. It can be contrasted to the Fourier transform, which has linearly spaced center frequencies $f_n = n * \frac{SR}{N}$, where $n$ is the frequency bin index, $SR$ is the sampling rate, and $N$ is the dimension of the transformation space. The \gls{cqt} instead has logarithmically spaced center frequencies $f_j = f_{min} * 2^{\frac{j}{b}}$ where $f_{min}$ is a pre-defined minimum frequency and $b$ determines the number of bins per octave. The fact that the center frequencies are logarithmically spaced results in the audio representation being translationally invariant, meaning that shifting a musical interval up or down will not change the distance between the harmonics in the \gls{cqt}. This enables a \gls{cnn} filter to discover harmonic patterns across the full range of frequencies. Another advantage of the \gls{cqt} representation is that its resolution resembles that of the human auditory system, with high resolution in the lower frequencies and wider bins in the higher frequencies. The downside of using \gls{cqt} is that it cannot benefit from the Fast Fourier transform optimization, so is computationally expensive, $\mathcal{O}(N^2)$ instead of $\mathcal{O}(N \log (N))$. Bittner \textit{et al.} add more resolution by computing multiple, overlapping \gls{cqt}s, each starting at a different frequency. This technique is called \gls{hcqt}. The \gls{cnn} structure includes four lower layers with small filters of dimension $5 \times 5$ or $3 \times 3$ to detect small-scale patterns. The fifth layer, instead, uses a filter that spans an octave of audio. This layer increases the relevant receptive field of each output state---the context in the input \gls{hcqt} it can access---without needing to make the network very deep. The sixth layer uses a $1 \times 1$ filter to combine all the learned features and output a pitch activation map \cite{bittner2017deep}. The \gls{dnn} proposed in this thesis utilizes CNN layers whose structure is closely based on this network. Since the pitch correction task is sensitive even to a small amount of pitch shift, I also choose to use the \gls{cqt} for its finer resolution in the lower frequencies. I do not use \gls{hcqt} as it is too computationally expensive.

Basaran {\it et al.} add a \gls{gru} layer \cite{chung2014empirical, ChoK2014arxiv} to the pitch detection \gls{cnn} described above in order to model the sequential nature of \cite{basaranmain} audio and music signals. The network estimates the main melody in polyphonic audio signals in the \gls{cqt} representation. I include a \gls{gru} in the proposed algorithm.

\subsection{Deep learning}
%deep learning and audio
%    - Amazon paper for CNN + rnn structure + pre-training (ok)
Research in the broader field of deep learning also provides a useful background for the given task. One challenge with the pitch correction \gls{dnn} is that its depth makes it hard to train. Wager \textit{et al.} improve the performance of a \gls{dnn} with multiple layers by first training the lower layers on a smaller task, then initializing the corresponding the full network with the trained weights. The network---designed for automatic speech recognition---has a similar structure to the one proposed in this thesis, with linear lower layers for feature extraction followed by recurrent layers for sequential processing \cite{wager2020fully}. I use a similar type of pre-training of a smaller version of the \gls{dnn} in the experiments described in this chapter. 
%    - WaveNet

\textit{Wavenet} is a highly expressive model that can synthesize or transform sound at the level of the sample \cite{oord2016wavenet}. The \textit{Wavenet}-based dereverberation network introduced in \cite{su2020hifi} demonstrates that it is able to learn a transformation for a highly complex task. The network is trained in an adversarial manner \cite{goodfellow2014generative} to help the results retain the nature of the original signal. It provides inspiration for moving to a sample-by-sample model from the current note-by-note model. Though this thesis does not include experiments using such a model, the concept of moving to a more fine-grained representation is worth investigating.

\subsection{Audio signal processing}
%pitch shifting
Monophonic pitch detection and transposition techniques provide tools for feature extraction and post-processing. I use the \gls{pyin} algorithm \cite{mauch2014pyin} for pitch detection in this thesis. \gls{pyin} is used as a benchmark for measuring frequency in monophonic music signals \cite{devaney2020new}. Unlike other state-of-the-art algoritms, including CREPE \cite{kim2018crepe}, its resolution is not limited to a margin such as 20 cents. While this precision is suitable for \gls{mir} tasks such as music recommendations, in the case of musical intonation, 20 cents can make the difference between being in tune or out of tune. The \gls{pyin} algorithm, like the pitch detection algorithm used for Antares Auto-Tune, is based on autocorrelation for periodicity detection. Autocorrelation alone is not always reliable: It might find a stronger periodicity at a harmonic---for example, the octave---or choose a maximum periodicity at value 0, when the signal is not shifted. The \gls{pyin} algorithm is based on the YIN pitch detection algorithm \cite{de2002yin}, which adds steps after the autocorrelation computation to reduce error from 10 per cent to 0.5 per cent. These steps include a weighting of the autocorrelation output to reduce periodicity at harmonics, and a cumulative normalization that discourages selection of the 0-lag periodicity without a need for an arbitrary threshold. It also includes linear interpolation to further refine the pitch estimate. The \gls{pyin} algorithm applies a \gls{hmm} to the output of YIN, using a set of candidate periodicities instead of selecting the maximum one. This results in a smoother output. Both YIN and \gls{pyin} output 0 when they do not detect a period, making the output suitable for voice activity detection and note boundary analysis. The precision of the pitch measurement combined with the voice activity detection make \gls{pyin} ideal for the automatic pitch correction task. As described in \ref{sec:notes}, the \gls{hmm} used in \gls{pyin} inspires the \gls{hmm} explored in this thesis for detecting note boundaries. 

%- PSOLA: TD-PSOLA algorithm 
Another useful tool is \gls{psola}, a pitch shifting algorithm \cite{charpentier1986diphone}. I use it in the post-processing phase, applying the shifts to the audio. Similarly to YIN, it starts by detecting periodicity in audio. It then splits the audio signal into individual periods, and shifts these slightly in time to produce the effect of shorter or longer periods. It uses cross-fading to avoid clipping. \gls{psola} is suitable for the task of applying pitch corrections because it produces a natural sounding result. Unlike other pitch-shifting techniques such as resampling, it does not modify the structure of the waveform except at the edges of audio windows. This minimizes changes to the formant---the harmonic structure of the sound---so that the timbre is not modified along with the pitch. 
% probabilistic YIN --> also inspires own approach to \gls{hmm} note parsing

\begin{figure*}[t]
\subfigure[]{\includegraphics[height=1.625in]{figures/cqt_comparison_1.pdf}}\hspace{-.15in}%\vspace{-.03in}
\subfigure[]{\includegraphics[height=1.625in]{figures/cqt_comparison_2.pdf}}\hspace{-.15in}%\vspace{-.03in}
\subfigure[]{\includegraphics[height=1.625in]{figures/cqt_comparison_3.pdf}}\hspace{-.15in}%\vspace{-.03in}
\subfigure[]{\includegraphics[height=1.625in]{figures/cqt_comparison_4-2.pdf}}\hspace{-.15in}%\vspace{-.03in}
\subfigure[]{\includegraphics[height=1.625in]{figures/cqt_comparison_5-2.pdf}}\hspace{-.15in}%\vspace{-.03in}
    \caption{
    \gls{cqt} of the vocals and backing tracks computed using Librosa \cite{mcfee2015librosa}. The plot focuses in on frequency bins 300 through 700 out of 1024 for better visibility. (a) shows the \gls{cqt} of the backing track. The horizontal lines are due to constant pitches, which indicates that a chord is being played. (b) and (c) show the \gls{cqt} of the vocals before and after the correction, respectively. (d) and (e) show the superposed vocals and backing track before and after corrections. The \gls{cqt}s are binarized by the mean of their amplitude, which makes the louder components stand out for visibility (see Section \ref{sec:data-format-autotune}). In this example, we see that the correction shifted the pitch of the vocals up and centered it around the desired harmonics of the backing track (red circles).
    }
    \label{fig:model-input-autotune}
\end{figure*}

%Proposed model
\section{The proposed algorithm}
\label{sec:proposed-autotune}
%- harmonic alignment (ok)

\begin{figure}[t!]
    \centering
    \includegraphics[width=12cm]{images/pitch_before_detuned_after.png}
    \caption{Training technique for the model using synthesized in tune versus out-of-tune data pairs. The program first detunes the original singing. As a result, the measured pitch moves from the purple line to the red line. The deep neural network takes as input the detuned signal, and predicts shifts that will restore the original pitch. The result of the predicted corrections is in green.}
    \label{fig:results}
\end{figure}

The proposed model takes as input two \gls{cqt}s---the monophonic vocal track's and the backing track's (also called accompaniment)---and outputs pitch shift predictions. The \gls{dnn} structure is built on the assumption that the backing track has clearly identifiable pitches---a chord progression---which serve as a reference for the vocals. The program uses the harmonic alignment between the vocals and backing tracks to make its predictions. Figure \ref{fig:model-input-autotune} shows the \gls{cqt} of vocals and backing track excerpts of a few notes before and after applying predicted pitch corrections. In the excerpt, the backing track pitches are mostly constant, meaning that a chord is likely being held. After the correction, the vocals appear to be more closely aligned with the backing track, as can be seen in a \gls{cqt} combining the two tracks.

%- supervised training pairs. discuss this in data sec. (ok)
The model is trained in a supervised manner. Training data consists of pairs of performances that are identical except for the vocals pitch. The backing track remains fixed, as it is when the singer performs in a karaoke setting. The input-target pairs, while required to train the model, are difficult to come across naturally. Hence, I synthesize them by detuning high-quality singing performances to construct the input signals, and then train the \gls{dnn} to predict the shifts that recover the original performances. Section \ref{sec:data-format-autotune} describes the de-tuning process.
\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{images/model_outline_extension.png}
    \caption{Model architecture with extension layer. A \gls{gru} sequentially processes the outputs for each note from the original \gls{dnn} and is followed by a linear layer that outputs note-wise shifts.}
    \label{fig:model_outline_extended}
\end{figure}
%    - applies to any culture it is trained on
%- time-frequency representation overview
%    - \gls{cqt}

%- note splitting
%    - Discretization for control, constant shift
\subsection{Note-by-note processing}
\label{sec:notes}
The network corrects the pitch of each note by shifting all frames included in it by a constant. This approach is based on the assumption that every note is a single musical event, and that its accuracy can be improved by shifting it as a unit. The first step to processing the performance note by note is to detect the note boundaries. I choose not to use a musical score, first, because this makes the program usable in the many situations where no score is available, second, because this avoids inconsistent note boundaries due to alignment errors or improvisation. I tested three different approaches to score-free detection of note boundaries. Their respective outputs can be visualized in Figure \ref{fig:note-parsing}.
\begin{figure}[t!]
    \centering
    \includegraphics[width=8cm]{images/note_detuning_hmm.png}
    \caption{Note de-tuning Hidden Markov Model. The approximate de-tuning amount per note is defined in the hidden states. The exact de-tuning is sampled from the state using a Gaussian distribution,.}
    \label{fig:detuning_hmm}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figures/note_parse_comparison_attention_5.eps}
    \caption{Comparison of three different note boundary detection outputs for an excerpt from \textit{Attention} by Charlie Puth. The purple line shows the \gls{pyin} frame-wise pitch contour. The vertical lines show the note boundaries. The first and second approaches use the frame-wise \gls{pyin} pitch output. The first approach assigns note boundaries at the beginnings and ends of unvoiced sections. The second approach fits a Gaussian \gls{hmm} to the pitch contour, and uses the hidden state sequence of equal-tempered scale frequencies along with unvoiced frames to assign boundaries. The third one uses the \gls{pyin} note-wise output. In this example, the unvoiced frame approach fails to split some \textit{legato} passages into individual notes and the \gls{pyin} note approach is the most sensitive, assigning the largest number of notes. Sometimes this is musically relevant: for example, the lyrics in the three-step descending sequence around frames 300 to 400 are ``knew-that-I, knew-that-I, knew-that-I'', splitting each step into three musical events. The \gls{pyin} note detection detects these boundaries. However, it misses some notes---for example, the first note after frame 600---and its boundaries are not exactly aligned with the frames that switch between being voiced and unvoiced---for example, in multiple locations between frames 800 and 900.}
    \label{fig:note-parsing}
\end{figure}

The first note parsing technique, which produced the best result, was to define every transition silence as a note boundary. To this end, I analyzed the vocals pitch using the frame-wise \gls{pyin} algorithm, implemented as a Vamp plugin in \cite{cannam2010sonic}. The frame-wise pitch is set to 0 for unvoiced frames, which makes it possible to easily treat transitions between voiced and unvoiced frames as note boundaries. A small amount of smoothing was required to avoid glitches in the case where a single frame had value 0. The advantage of this technique is that it minimizes artifacts during shifting. Any discontinuities arising due to pitch shifting a section of an audio recording are insignificant because the audio is silent at the discontinuity points. The disadvantage is that this note parsing technique fails to split notes when they are connected, though this is common in \textit{legato} melodies. This means that if one part of a legato passage is out of tune and another part isn't, a shift that would correct the out-of-tune part would de-tune the remaining part.

The second approach, which turned out to be too error-prone for the task of automatic pitch correction, was to use the note-wise \gls{pyin} plugin available as an extension to the original frame-level pitch detection algorithm algorithm. This program is useful for melody detection, but I found that it did not always start and end notes at exactly the right frame for minimizing discontinuity. It often left small gaps between the end of one note and the beginning of the next one, making it difficult to determine what to do with these unaccounted frames. It also tended to be rather sensitive, converting pitch bending into two discrete notes. Any undesirable splitting of notes can have a significant impact on the ability of the model to perform reliably well. If a part of a note is separated from the rest and shifted in a different direction, the unnatural result can be displeasing enough to make a whole excerpt sound bad to the listeners.

The third approach assigns note boundaries to the vocal track using a Gaussian \gls{hmm} applied to the frame-wise \gls{pyin} pitch contour. This approach combines the reliable output of \gls{pyin} with a customized note parsing technique. The hidden states in the \gls{hmm} are the equal-tempered scale frequencies within the range of given performance, $440 * 2^{\frac{p - 69}{12}}$, where $p \in [21, 108]$, the standard \gls{midi} range. 0 is included as a state for silence or unvoiced frames. The HMM standard deviations map to the difference in pitch between each equal-tempered scale frequency, leaving much room for the pitch in a note to vary from the center mean. The transition matrix is set to 0.001 everywhere except in the diagonal, which makes each row sum to 1. The starting probabilities are uniformly distributed. Fitting this model assigns a \textit{note} state to each frame and provides boundaries both between \textit{legato} notes and between unvoiced and voiced sections. The potential shortcoming of this approach is that it relies on the definition of a discretized scale. I note that it would be possible to use a more fine grained scale if working with a musical style that is not based on the equal-tempered scale---for example, use 22 subdivision of the octave for Classical Indian music. I used the equal-tempered scale here because the dataset I worked with was mostly made of popular music that used that scale.  

%    - hmm for note splitting (ok)
%        - now set up for equal-tempered for simplicity (ok)
%        - could set up a fine-grained scale, 22 notes as %in raga, or any other customization (ok)
% could, in the future, try WaveNet (ok)
%- model structure with CNN and RNN
\subsection{Neural network structure}
I trained two different \gls{dnn} architectures. The second is an extension of the first, designed to include more temporal context across notes. The first version of the network consists of six stacked convolutional layers followed by a \gls{gru} layer. The network architecture is illustrated in Figure \ref{fig:model_outline}. The last output of the \gls{gru} is fed to a dense layer that predicts a single scalar output, the note-wise pitch shift. The convolutional filters pre-process the \gls{cqt}, reducing its dimensionality while also extracting abstract features. Next, the \gls{gru}---from which the network only uses the last output---reduces the representation of a variable-length note to a fixed-length vector. Finally, the dense layer predicts the pitch shift in the approximate range of $-1$ to $1$, which is mapped up to a semitone in either direction, or $-100$ to $100$ cents. A semitone corresponds to one note shift on a piano. 

The activation after each convolutional layer is the \gls{relu} \cite{he2015delving}. The \gls{dnn} uses a linear activation for the prediction, to prevent having the model converge to favoring larger shifts. For example, the hyperbolic tangent function might have tended to move values closer to $-1$ or $1$ and away from 0.

The \gls{gru} recurrent structure is a way for the model to analyze the singer's note contour, which can last from a split second to multiple seconds, while smoothing over unvoiced or noisy sections. This is crucial because the algorithm is expected to rely on aligning harmonics, which only occur in pitched sounds. Another advantage of using the \gls{gru} is that the hidden state output by one note can initialize the hidden state for the following note, passing along some information about the previous notes. Even when using the simplest possible detuning model, which shifts every pitch by an independent amount, we can assume that some information from past notes (e.g. from the backing track) is useful. The \gls{gru} is unidirectional, meaning that it is only exposed to past musical events. I assume this is sufficient information, as a performing musician can adjust intonation without hearing future events. 

\begin{table}[t]
  \begin{center}
    \caption{The proposed network architecture.}
    \begin{tabular}{|c||c|c|c|c|}
    \hline
      & Conv1 & Conv2 & Conv3 & Conv4 \\
      \hline
      \#Filters/Units & 128 & 64 & 64 & 64 \\
      Filter size & (5, 5) & (5, 5) & (3, 3) & (3, 3) \\
      Stride & (1, 2) & (1, 2) & (2, 2) & (1, 1) \\
      Padding & (2, 2) & (2, 2) & (1, 1) & (1, 1) \\
      \hline
      & Conv5 & Conv6 & \gls{gru} & Linear \\
      \hline
      \#Filters/Units & 8 & 1 & 64 & 1 \\
      Filter size & (48, 1) & (1, 1) & & \\
      Stride & (1, 1) & (1, 1) & & \\
      Padding & (24, 1) & (0, 0) & & \\
      \hline
    \end{tabular}
    \vspace{-.1in}
    \label{tab:network}
  \end{center}
\end{table}

%- option of adding additional song-rnn
The extended version of the \gls{dnn} is designed to enable the model to include more temporal context. As shown in Figure \ref{fig:model_outline_extended}, the final dense layer is replaced by a second \gls{gru} that takes as input the sequence of note representations output one-by-one by the first \gls{gru}. It finally applies a dense layer to the full sequence to output a song-level prediction sequence. This version has the potential to reach farther back in time, and include information from the chord progression in the backing track and long-term melodic patterns in the vocals. The downside of this model design is that the \gls{dnn} is deeper, which makes it more difficult to train. 

% - predict shift, apply it later (ok)
Table \ref{tab:network} displays the structure of the proposed network without the extension. The feature extraction layers are convolutional, which is common for image processing. The input to the model is in spectrogram format, which resembles an image, except for the fact that its meaning is different along the time and frequency axes. In image processing, dimensions reduction techniques like max pooling are common. These techniques treat the x and y axes in the same way, which we wish to avoid. To preserve the frequency patterns axis, the proposed approach instead uses strides of two only in the time axis in three of the convolutional layers. This method was shown successful for the task of learning latent representations for speech generation and transformation in \cite{hsu2017learning}. The third layer also includes a stride along the frequency axis, but this occurs only in one layer to not lose too much information. The fifth convolutional layer has a filter of size 48 in the frequency domain, which captures frequency relationships in a larger range of the \gls{cqt}, as done in \cite{bittner2017deep} and \cite{hsu2017learning}. 
The error function is the \gls{mse} between the pitch shift estimate and ground truth over the full sequence of notes. The \gls{mse} corresponds to the error in cents using the formula $\left|\text{cent error}\right| = 100 * \sqrt{\text{\gls{mse}}}$. % When training and testing the program, \gls{mse} is the only objective accuracy metric, though we have audio examples for an informal evaluation. A subjective test provides a qualitative evaluation when applying automatic pitch correction to real-world singing examples.

\subsubsection{Applying the predicted pitch corrections}
Once the program has output pitch correction predictions, these are applied to the vocals in post processing. The \gls{psola} algorithm provides a natural sounding output with few artifacts. The shifting is constant across note, and subtle cross-fading is applied between legato notes---in between which there is no silence---to avoid glitches at the boundaries.

%- post-processing focus
\subsection{Post processing versus real time}
The neural network introduced in this thesis is not explicitly designed to work in real time. First, the algorithm is designed for post-processing plug-ins such as are found in music apps like Smule. Second, the task of automatic pitch correction is challenging enough even given abundant computation time, so I choose to leave real-time applications to future work. 

Even in its current form, though, the model might be adaptable to near-real-time processing as it only uses information from previous notes to make a prediction for the current note. Even within a note, it processes one frame at a time in order. The challenge would be to make the data pre-processing---involving pitch detection and feature extraction---fast enough. 

%Dataset
%- Intonation introduction, refer to chapter (ok)
%    - genres in the dataset (ok)
%Real-world dataset (ok)
%- real-world set for testing (ok)
\subsection{Dataset}
\label{sec:dataset-autotune}
During my internship with Smule, Inc, a company that offers a singing app for smartphone, my team and I constructed a training dataset by deriving from the ``Intonation" dataset \cite{wager2018intonation}, which is assumed to be a collection of in-tune singing voice tracks. A detailed description of the dataset, including instructions on how to access it, is available in Chapter \ref{chap:thesis-damp}. The 4702 separate vocal tracks in the dataset are mostly of Western popular music, collected at Smule for good intonation. While browsing the dataset, I also discovered a few tracks of Blues; Western Classical music; Latin, Japanese, and Indian popular music, Country; and Rock. The songs are mostly based on the equal-tempered scale, but contain a wide variety of pitch deviation patterns from the scale. I discuss the measured pitch distribution in Chapter \ref{chap:thesis-damp}. While these real-world recordings contain some artifacts, no particular signal processing---e.g. denoising or filtering---has been applied to them. Each recording contains one minute of a performance, starting 30 seconds into the song. Although they are assumed to be in tune, this is not always exactly the case as the users are not necessarily professional singers. Overall, the sung pitch is sounds quite accurate and aligns reasonably well in timing and in pitch with the known musical score. Note when compared with the intended pitch. Hence, we can treat this paper as a proof of concept. The model can be trained on professional singing for best results.

Based on the metadata for each track indicating the backing track and user index, the dataset is split into 4561 training performances, 49 validation performances, and 64 test performances. The training set contains 709 backing tracks performed by 3468 different users, while the validation set is with 17 tracks sung by 43 users and the test set is with 16 sung by 62. There is no overlap in the backing tracks across the three sets. Overlap exists in the singer ID between the training and validation sets, but not with the test set. 

%For a subjective listening test, I also create another real-world test set using the test backing tracks. Originally, 8 volunteers sang along with them outside of Smule. Singing experience ranged from beginner to semi-professional. The singers chose what to sing, and selected a total of 9 different arrangements. The dataset consisted of a total of 24 performances. The dataset later grew to N performances to increase its size and diversity of voices and singing styles. Instructions during the recording session included singing once in a normal manner and once mimicking an imaginary out-of-tune karaoke singer. The reason for this is that it is interesting to examine how the algorithm performs both with performances that are already quite accurate, and with performances that are severely out of tune. Singers familiarized themselves with their chosen songs at their own pace over the course of a few days before the recording session. They used the Smule interface, which provides the lyrics and the piano roll score for reference. During the performance, they listened to the backing track through headphones so that it would not interfere with the vocals recording.

%De-tuning process
\subsection{The detuning process}
As introduced in Section \ref{sec:proposed-autotune}, the \gls{dnn} is trained in a supervised manner. An input data sample consists of an out-of-tune performance, and the target consists of the note-wise shifts that should be applied to the vocals track to make it sound in tune. This type of pair is difficult to come across, unless one manually labels the corrections for every note in hundreds or in thousands of performances. The proposed approach to constructing training examples is to synthesize de-tuned examples. Singing performances from the ``Intonation'' dataset are de-tuned by shifting every note up or down and recording the amount of shift as the target. The synthetic pitch deviations are limited to approximately one semitone (100 cents) in either direction, a larger interval than the standard score-free approach of snapping to the nearest pitch, which limits the shift to 50 cents. In practice, it prevents errors in cases where the required shift is greater than 50, but can lead to degradation of the prediction accuracy on a too badly detuned input. 

To detune the training data, the program shifts the magnitude \gls{cqt} up or down. This is expected to not produce too noticeable artifacts that the program could learn instead of the pitch relationships. The one issue is formant shifting, but this is not a big concern when only shifting by $\pm$100 cents or less. I experimented with shifting the training data using \gls{psola}, but this did not produce noticeably better convergence, and increased the computational complexity due to the need to compute autocorrelation instead of simply shifting a matrix. 

\subsubsection{De-tuning distributions}
%- data de-tuning, different versions (ok)
%    - runif (ok)
%    - hmm (ok)
%        - MIR-1K (ok)
% rnn (ok)
The de-tuning process described in the previous section requires assigning a distribution to the random shifts. Choosing a proper distribution involves balancing exposing the model to a wide variety of errors with ensuring it also is exposed to small deviations. A too spread distribution risks causing the model to frequenly apply large corrections, and produce noticeable errors. 

I experimented with two distributions. The first is the random uniform distribution in the range [-100, 100] cents, adjusted to the logarithmic scale of cents so that the shift of the \gls{cqt} spectrogram is linear. Random uniformly distributed shifts are very simple to implement, and provide the model with plenty of examples of a wide variety of shifts, ranging from zero---which is useful, because singers are not likely to sing every note out of tune---to large ones. The downside is that this de-tuning technique is not based on real intonation patterns in out-of-tune singing. Furthermore, it is based on the strong and likely incorrect assumption that the detuning for every note is independent from that of other notes.

A Gaussian \gls{hmm} addresses both of the shortcomings of the random uniform distribution in a simple manner. \gls{hmm} states represent deviation levels, while the Gaussian distribution provides variance. The \gls{hmm} structure is illustrated in Figure \ref{fig:detuning_hmm}. The \gls{hmm} is trained on real-world singing examples in the publicly available MIR-1K dataset of karaoke performances \cite{hsu2009improvement}. The proposed \gls{hmm} outputs deviations in cents from the equal-tempered scale. These deviations are represented as the difference between the equal-tempered frequency output by the \gls{hmm} used to parse notes, described in Section \ref{sec:notes}, and the median of the measured frame-wide \gls{pyin} pitch. As before, the equal-tempered scale can be replaced with any other division of the octave. The proposed model uses five hidden states, but the number is arbitrarily chosen and can be replaced by a different value.  Sampling from this \gls{hmm} produces sequences of pitch deviations that are based on real-world deviations and are not completely independent across notes.

To address the issue that all deviations will be 50 cents or less, because the distance to the nearest scale degree will never be greater than this, the \gls{midi} pitch is moved by a semitone 5 per cent of the time across the median measured pitch, increasing the range to 100 cents. 5 per cent was another arbitrary choice, but the use of \gls{hmm} is still expected to produce a slightly more accurate pitch behavior representation than the random uniform distribution.

The parameters learned from the MIR-1K dataset are in Table \ref{tab:detuning-hmm}. The means show a state very close to the equal-tempered scale, at 3, two that are offset by a few cents---one in each direction---and two that are more than a quarter tone away. I note that singers were very likely to start close to an equal-tempered scale degree, and almost never started far off. They also didn't tend to stay far off, as the transition probabilities from a larger deviation to a larger deviation are the smallest.\footnote{I later realized that the pitch detection was based on mixed MIR-1K signals, which included the accompaniment. This resulted in a more spread and noisy distribution, but I believe this larger spread was useful for training the model to address larger shifts.} One should note that these parameters include the occasional shifts of the note across the median pitch. Despite this modification, the distribution is slightly less spread than the random uniform option, as shows in Chapter \ref{chap:results}, Figure \ref{fig:test-comparison}.

\begin{table}[t]
  \begin{center}
    \begin{tabular}{|c|c|c|c|}
    \hline
      $\mu$ & $\sigma$ & $p_{start}$ & $p_{trans}$ \\
      \hline
      &&& \\
      $\left[ \begin{array}{c} 52 \\  13 \\3\\-8\\-73 \end{array}\right]$
      & $\left[ \begin{array}{c} 31  \\ 25\\16\\25\\24 \end{array}\right]$
      & $\left[ \begin{array}{c} 0.1  \\ 9\\76\\15\\0.1 \end{array}\right]$
      & $\left[ \begin{array}{ccccc} 4 & 30 & 30 & 35 & 1  \\ 13&27&30&24&5\\24&17&22&15&23\\16&25&28&23&7\\3&36&30&30&0 \end{array}\right]$  \\
      &&& \\
      \hline
    \end{tabular}
    \label{tab:detuning-hmm}
    \caption{The de-tuning Gaussian \gls{hmm} parameters fitted to the MIR-1K dataset. The first column shows the means, or hidden states, and the second column shows the standard deviations. The final two columns show the start and transition probabilities. All parameters are in cents, a logarithmic measure, and rounded to the nearest integer, except for zeros, which are set to $0.1$ to show that no transition had zero probability.}
  \end{center}
\end{table}

The \gls{hmm} model pitch deviation is not the most complex model that could be used for the task. It could be replaced by a RNN that uses additional information, such as absolute frequency---given that a singer might be more likely to sing sharp or flat based on the register---or spectral information. One could go another step further and synthesize out-of-tune singing using a WaveNet and/or adversarial training.

\subsection{Data pre-processing details}
\label{sec:data-format-autotune}
The audio signals are normalized, then transformed using the \gls{cqt}. The \gls{cqt} covers 5.5 octaves with a resolution of 16 bins per semitone. The lowest frequency is 125 Hz. The top and bottom 16 bins are used as a buffer for pitch shifting, then truncated so that every input has dimension 1024. The frame size spans 92 ms and the hop size 11 ms. The vocals and backing track \gls{cqt}s form two input channels to the neural network. In previous work, I experimented with using a third channel that would help bring out the contrast between the first two channels. I binarized the two \gls{cqt} spectrograms using the mean modulus as a threshold, a technique used in computer vision \cite{sezgin2004survey}. I then took the bitwise disagreement of the two matrices based on the expectation that the in-tune singing voice, better aligned with the backing track, would cancel out more harmonic peaks than the out-of-tune tracks. Figure \ref{fig:model-input-autotune} illustrates the three channels. It includes the binarized \gls{cqt} before and after shifting though the third channel was ultimately left out, because it helps visualize the shift. Surprisingly, though the convergence with the third channel was better for multiple epochs, the loss with two channels ultimately dropped below its counterpart. The difference took long enough to appear that I only discovered it because I had left two models training for additional time despite having already concluded that the third channel improved the results. 
%- \gls{cqt} parameters, etc… (ok)
%- tried third channel, did not help (ok)
%- tried the binary channel but did not help (plot)? (ok)
%- Tried \gls{psola}, expensive but not better (ok)

%Experimental setup
\section{Experimental configuration}
\label{sec:experiments-autotune}
\subsection{Training setup}
The program uses the Adam optimizer \cite{kingma2014adam}. It processes one note at a time as a minibatch of seven differently pitch-shifted versions. It does not include batch normalization because the different versions of the same note are not \textit{i.i.d}. When using the second \gls{gru} layer that bases the prediction on the sequence of notes, the outputs for each note are stored, then input to the \gls{gru}. 

The program applies gradient clipping \cite{pascanu2013difficulty} with a threshold of 100. It reports validation loss every 500 songs and save the model with the best result along with the latest one. 
%- loss function, etc… (ok)
%- Adam (ok)
\subsection{Initialization}
The convolutional parameters are initialized using He \cite{he2015delving}, and the \gls{gru} hidden state of the first note of every song is initialized using a normal distribution with $\mu=0$ and $sd=0.0001$. The hidden state of the note sequence \gls{gru} is initialized in the same way. When using the note sequence \gls{gru}, the weights from the note-by-note model can optionally be used to initialize the lower layers. These lower layers can also be fixed for an epoch before being trained all the network parameters. 

\subsection{Experiments}
\label{sec:experiment-list}
In this thesis, I report test results on a set of different configurations, listed in Table \ref{table:experiments}. Note that I only report configurations that showed the strongest convergence. First, I compare note parsing techniques and de-tuning techniques when training the smaller version of the model. I examine various learning rates. For the best performing models, I add the song-level \gls{gru} extension to check whether the deeper neural network performs better. I train the extended model either from scratch or initializing feature extraction parameters with the values learned for the smaller model. In the latter case, I freeze the pre-trained weights for one epoch. This technique was shown successful in previous work, e.g., \cite{wager2020fully}. For each model, I report results either with learning rate 0.00001 or 0.000005, based on which setting converged best. Other learning rates did not converge.

\begin{table*}
\centering
\begin{tabularx}{\columnwidth}{ |X|X|X|X|X| } 
\hline
\multicolumn{5}{|c|}{\textbf{Experiment settings}}\\
\hline\hline
\textbf{Note parsing} & \textbf{De-tuning} & \textbf{Learning rate} & \textbf{Extension} & \textbf{Initialization} \\
\hline\hline
Silence & Uniform & 0.00001 & No & He, Gauss \\
\hline
Silence & \gls{hmm} & 0.000005 & No & He, Gauss\\ 
\hline
\gls{hmm} & \gls{hmm} & 0.00001 & No & He, Gauss\\ 
\hline
Silence & Uniform & 0.000005 & Yes & pre-trained, Gauss\\
\hline
Silence & \gls{hmm} & 0.000005 & Yes & He, Gauss\\ 
\hline
Silence & \gls{hmm} & 0.000005 & Yes & pre-trained, Gauss\\ 
\hline
\end{tabularx}
\label{table:experiments}
\caption{The \textit{Note parsing} column indicates whether the note boundaries were assigned based on silent \gls{pyin} frames or based on state changes in the \gls{hmm} assigning a scale degree to each frame. The \textit{De-tuning} column indicates whether the de-tuning distribution was random uniform or sampled from the \gls{hmm} trained on MIR-1K. The \textit{Extension} refers to whether the song-level \gls{gru} is added to the model architecture. Finally, the \textit{Initialization} column provides the distributions used to initialize the parameters, and whether the feature extraction layers were initialized using pre-trained parameters from the model without extension.}
\end{table*}

%\begin{table*}
%\centering
%\begin{tabularx}{\columnwidth}{ |X|X|X|X| } 
%\hline
%\multicolumn{4}{|c|}{\textbf{Experiment settings}}\\
%\hline\hline
%\textbf{Model names} & \textbf{Learning rate} & \textbf{Initialization} & \textbf{De-tuning} \\
%\hline\hline
%Note-runif-1e-5 & 0.00001  & He, Gauss & Uniform \\
%\hline
%Note-runif-5e-6 & 0.000005 & He, Gauss & Uniform \\
%\hline
%Note-\gls{hmm}-1e-5 & 0.00001 & He, Gauss & \gls{hmm} \\ 
%\hline
%Song-runif-1e-5 &  0.00001 & He, Gauss & Uniform \\ 
%\hline
%Song-runif-5e-6 &  0.000005 & He, Gauss& Uniform \\ 
%\hline
%Song-\gls{hmm}-1e-5 & 0.00001 & He, Gauss & \gls{hmm} \\ 
%\hline
%Song-\gls{hmm}-5e-6 & 0.000005 & He, Gauss & \gls{hmm} \\ 
%\hline
%Song-\gls{hmm}-pretrain-1e-5 & 0.00001 & ``Note-\gls{hmm}-1e-5'' weights, Gauss & \gls{hmm} \\ 
%\hline
%Song-runif-pretrain-1e-5 & 0.00001 & ``Note-\gls{hmm}-1e-5'' weights, Gauss & Uniform\\
%\hline
%\end{tabularx}
%\label{table:experiments}
%\caption{List of experiments}
%\end{table*}

%- initialization of weights
%- pre-training
%Experiments
%- runif by note
%- hmm by note
%- runif by sequence
%- hmm by sequence
%- learning rates
%- + pre-training