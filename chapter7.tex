\chapter{Conclusion and future work}
\label{chap:conclusion}

This thesis presents a first iteration of a data-driven algorithm for estimating and applying pitch corrections in a monophonic vocal track, while using the instrumental accompaniment track as reference. The \gls{dnn} used to make pitch correction predictions is exposed to both incorrect intonation, for which it learns a correction, and intentional pitch variation, which it learns to preserve. It treats pitch as a continuous value rather than a discrete set of notes. It does not rely on a musical score, thus allowing for improvisation and harmonization in the singing performance. Results on a convolutional neural network with a gated recurrent unit layer indicate that spectral information in the accompaniment and vocal tracks is useful for determining the amount of pitch correction required at a note level. 

The fact that musical pitch isn't represented as a discrete set of notes also means that it can be adapted to many different musical traditions, regardless of the scales and pitch variation patterns used in these traditions. Though the network was trained in this thesis work on Western popular music---as this was the data that was available---it can be adapted to other genres and cultures by training on the appropriate data.

More generally, comparison of the pitch behavior in the ground-truth ``Intonation'' dataset to a small, professional dataset shows a large difference distributions. Also for this reason, the results described in this thesis are prototypical in nature, and one of the challenges for future work is to utilize professional-level data. 

The current model is built on some strong assumptions. First, that the backing track has clearly identifiable pitches---a chord progression---which serves as a reference for the vocals. Second, that a singer targets a specific frequency per note, around which all pitch variations are centered. Moving beyond these assumptions can make the model usable in more contexts or make it more accurate in currently applicable contexts. While the current model outputs the amount by which singing should be shifted in pitch, the model can be extended to predicted the pitch-shifted signal directly in an end-to-end model. The proposed algorithm makes mistakes. Making it usable in practice requires collaboration with human-computer interaction designers, music theorists, and musicologists.
