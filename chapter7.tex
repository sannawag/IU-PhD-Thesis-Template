\chapter{Conclusion and future work}
\label{chap:conclusion}

This thesis presents a the first iteration of a data-driven deep learning model for estimating pitch correction for a monophonic vocal track using the instrumental accompaniment track as reference. The system is exposed to both incorrect intonation, for which it learns a correction, and intentional pitch variation, which it learns to preserve. It treats pitch as a continuous value rather than a discrete set of notes and does not rely on a musical score, thus allowing for improvisation and harmonization in the singing performance. Results on a CNN with a GRU layer indicate that spectral information in the accompaniment and vocal tracks is useful for determining the amount of pitch correction required at a note level. 

The fact that the is not bound to a discrete set of notes also means that it can be adapted to many different musical cultures, regardless of the scales used in these cultures. Though the network was trained on Western popular music in this thesis, as this was the data that was available, it can be adapted to many musical cultures by training on the appropriate data.

The current approaches for generating out-of-tune singing can be improved in future work. A recurrent neural network or adversarial training could result in a more natural-sounding result. The current model is built on some strong assumptions. First, that the backing track has clearly identifiable pitches---a chord progression---which serves as a reference for the vocals. Second, that a singer targets a specific frequency per note, around which all pitch variations are centered. Moving beyond these assumptions can make the model usable in more contexts or make it more accurate in currently applicable contexts. While the current model outputs the amount by which singing should be shifted in pitch, the model can be extended to predicted the pitch-shifted signal directly in an end-to-end model. The proposed system makes mistakes. Making it usable in practice requires collaboration with human-computer interaction designers, music theorists, and musicologists.