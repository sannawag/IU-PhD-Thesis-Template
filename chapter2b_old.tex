\chapter{Designing technology for music: intervention-based design}
\label{chap:ibr}
Chapter \ref{chap:intonation} discusses the design of an autotuner from a musical standpoint. Ultimately, the quality of the proposed autotuner doesn't only depend on the sophistication of the assumptions we make about musical intonation and on the features we use to make predictions: It also depends on how well these concepts can be implemented as a computer program. This chapter focuses on the technical aspect. It focuses, first, on music and audio representation capability in software, second, on the current state of machine and deep learning and how these can be applied to the task.

Technical writer Jaron Lanier wrote in 2010 about the invention of the \textit{Musical Instrument Digital Interface}, or \textit{MIDI}. ``One day in the early 1980s, a music synthesizer designer named Dave Smith casually made up a way to represent musical notes. It was called MIDI. His approach conceived of music from a keyboard player's point of view. MIDI was made of digital patterns that represented keyboard events like ``key-down'' and ``key-up''. 

That meant it could not describe the curvy, transient expressions a singer or a saxophone player can produce. It could only describe the tile mosaic world of the keyboardist, not the watercolor world of the violin. But there was no reason for MIDI to be concerned with the whole of musical expression, since Dave only wanted to connect some synthesizers together so that you would have a larger palette of sounds while playing a single keyboard. In spite of its limitations, MIDI became the standard scheme to represent music in software. Music programs and synthesizers were designed to work with it, and it quickly proved impractical to change or dispose of all the software and hardware. MIDI became entrenched, and despite Herculean efforts to reform its on many occasions by a multi-decade-long parade of powerful international commercial, academic and professional organizations, it remains so.'' \cite{lanier2010you}

Between 2010 and 2020, technology has rapidly developed, and the state of music and audio technology does not look quite as bleak. MIDI is still a very common tool. Some technical lock-in still exists. An overarching theme in this chapter is that we can value a technological invention such as MIDI as powerful tool while also considering what can be either improved or fundamentally changed. Similarly to the ratio conceptualization of musical intervals discussed in \ref{sec:ratio}, MIDI provides a discretization and simplification of music that is elegant and easy to work with, but also risks discarding nuances. 

MIDI is not the only music representation. This chapter considers some developments that enable a more complex music representation. This chapter places music technology development into the framework of intervention based research (IBR), where ``confronting [an assumption] with reality has been shown to produce knowledge'' \cite{chandrasekaran2020ibr}. The way an invention in audio technology is received and affects how musicians and listeners relate to music can lead us to re-frame the way we think about music and build technology for music.

\section{Intervention Based Research}
Chandrasekaran \textit{et al} \cite{chandrasekaran2020ibr} describe how IBR is conducted when we use a method \textbf{M} to apply a basket of knowledge \textbf{T}\footnote{The original publication uses the term \textit{basket of theories}, but I use \textit{knowledge} to avoid confusion given other uses of the word \textit{theory} in this text}---including theories, descriptions, assumptions, recipes, typologies, categories, and constraints---to bring a problem situation \textbf{S} to an outcome \textbf{S*}. An unexpected result or failed intervention triggers abductive reasoning, which leads to adjustments to \textbf{M} or \textbf{T}. In abductive reasoning, the designer uses his or her knowledge of the world infer the best explanation for what happened. In this flexible framework, the conclusion does not need to follow logically from the premises---as would be the case in deductive reasoning---or be based on statistical data---as would be the case in inductive reasoning. It simply needs to be the most plausible explanation\cite{sep-abduction}. Abductive reasoning provides a way to deal with complexity in real-world systems without having a fully formed theoretical understanding of how they work. Sometimes, what emerges is that \textbf{S*}---where everyone had agreed to head---turns out to be the wrong place to go \cite{chandrasekaran2020ibr}. Unexpected events reveal the ill-defined nature of the problem, leading to a reframing activity. This process can be repeated multiple times, leading to an iteration between assumptions and evidence.

As I frame the development of an autotuner program in the context of IBR, questions come up regarding what to put into the basket of knowledge. Should I use a theory of musical intonation? Or stick to description, e.g., empirically derived information? What are my assumptions about musical intonation? What are the constraints for the program? What do I assume about the user base---what is the scope of the program? What recipes, e.g., subtasks such as feature extraction techniques, is the program built on? I will explore these questions and many more in this chapter. 

\subsection{A theory of musical intonation?}
This section provides a brief overview of literature on the concept of a theory, its advantages, and its limitations. It then refers to chapter \ref{chap:intonation} to argue that the Pythagorean and Just ratio-based conceptualizations of intonation are theories. Given that Antares Auto-Tune is built on this theory, for comparison, this section explores what a theory would look like for the proposed approach, which involves psycho-cultural phenomena. 

\subsubsection{Theory: A brief overview}
\label{sec:theory}
Bacharach defines theory as a ``statement of relations among concepts within a set of boundary assumptions and constraints''. A theory helps us ``organize (parsimoniously) and communicate (clearly).'' \cite{bacharach1989organizational} Its function ``is that of preventing the observer from being dazzled by the full-blown complexity of natural or concrete events.'' \cite{hall1957theories} Furthermore, it moves beyond simple description, enabling explanation and prediction of events: It focuses on questions How, Why, When instead of What. \cite{bacharach1989organizational}. Theories can be elegant in their simplicity and explanatory power. Developing a theory, however, often involves a trade-off. Bacharach writes: ``all theories are constrained by their specific critical bounding assumptions. These assumptions include the implicit values of the theorist and the often explicit restrictions regarding space and time.'' Furthermore, a theory that generalizes well ``[require] a higher level of abstraction, which means that [it] sacrifices the level of detail needed to fit a specific situation. Similarly, Van de Ven describes how increasing a theory's internal consistency often comes at the expense of limiting its scope, quoting Poggi: ``A way of seeing is a way of not seeing.'' \cite{poggi1965main} Micro logic leads to macro nonsense \cite{van1989nothing}. Bacharach also describes how falsifiability is important when building a theory. 

Pythagorean and Just conceptualizations of musical intervals as ratios can be formulated as a theory, answering questions \textit{How}, \textit{Why}, and \textit{When}. Good intonation is produced between two notes \textit{when} they are played consecutively or simultaneously and both are audible \textit{by} shifting their fundamental frequencies so that their ratio is one of a set of specified values: 1:1, 2:1, 3:2, etc...\textit{because} these ratios minimize shared period length, eliminating roughness and beats. This theory is very elegant and provides a high level of abstraction. For example, it applies to all musical contexts and all timbres. Furthermore, it can (and has) been partially falsified given empirical studies of musical intonation and the mathematical. 

\subsubsection{Auto-Tuner's basket of knowledge}
In section \ref{sec:interval-conceptualization-design}, which describes in detail the assumptions behind Auto-Tune, I argue that Auto-Tune's design is based on the ratio conceptualization of musical intervals. This means that its basket of knowledge includes at least one theory. However, the theory alone is not sufficient for implementing the program. Implementing ratio theory is often mathematically impossible, as described in \ref{sec:ratio}. To fix this issue, Auto-Tune's basket of knowledge includes the recipe of moving from Pythagorean and Just intonation to the equal-tempered scale: a compromise that makes the theory usable in practice, while sacrificing some of the purity of the intervals. Other items in Auto-Tune's basket of knowledge are the assumption that the musician wishes to be in tune as defined by ratio theory, and the constraint that the musician is using the equal-tempered scale or another fixed scale.

\subsubsection{Basket of knowledge for the proposed approach}
I now consider what a theory would look like if basing it on results from empirical studies of intonation. The number of variables increases---or explodes. I list some possible features that might be relevant to determining whether a note is likely to sound in tune as judged by a listener. The list is not comprehensive, but provides a general idea.

The first set of features relates to the music. It includes the genre of the piece being performed (including stylistic choices common for the genre, such as pitch bending), backing track instrumentation (number of instruments, timbre, relative amplitude), global and local tempo, duration of the given note, harmonic and melodic context.

The second set of features relates to the singer. It includes vocal type (soprano, alto, tenor, bass), vocal characteristics (timbre, breathiness) fundamental frequency characteristics (vibrato rate, vibrato depth, and jitter \cite{devaney2020new}), vocal training (years of training, genre e.g., classical, jazz, rock), general musical training (not related to singing), vocal ability (range, stability, expressiveness), personal aesthetics, amount of vibrato, ADSR envelope of the given note, musical choices and successful and undesired outcomes during the note and earlier in the performance.

The third set of features relates to the listener. It includes musical background (musical genre familiarity and affinity, culture, musical training, hearing ability, subjective preference), audio quality (live, analog, digital, compression, sample rate, bit depth, speaker quality, number of channels), room acoustics (reverberance, distance from the source to the listener), perceived intonation accuracy of previous notes, relationship of listener to performer (self, teacher, parent, fan, audience member who bought an expensive ticket).

More generally, the theory should include aspects of what is known about the the physics of sound and psychoacoustics, cochlea structure, masking, etc...

Given all of these features, many of which are hard to quantify or measure objectively, and their complex interactions, a theory that encompasses the psycho-cultural aspect of musical intonation is challenging to develop. However, I argue that much work towards developing an autotuner can be done without developing a comprehensive theory. I do so in the following section by placing this work into Bohn's framework for measuring and managing technological knowledge \cite{bohn1998measuring}.

\section{Stages of knowledge}
Bohn identifies eight stages of technological knowledge, ranging from complete ignorance to complete understanding. His writing is in the context of manufacturing, but can be applied to music software in a straightforward manner. I summarize the stages of knowledge here but encourage the reader to refer to the paper for more details \cite{bohn1998measuring}. As I introduce each stage of knowledge, I also relate it to autotuning.

\textit{Stage One---Complete ignorance} You do not know that a phenomenon exists or that it is relevant. You cannot hear musical intonation.
\textit{Stage Two---Awareness} You know of the phenomenon but are not sure how to use it. At this point, you can start investigating it. You notice that some musical performances are consonant and others are dissonant.
\textit{Stage Three---Measure}
You can measure variables and see how they relate to the phenomenon, but not control them. You can investigate the relationship between fundamental frequency, timbre, and musical intonation.
\textit{Stage Four---Control of the mean}
You can stabilize the process you developed when variables are near the mean. You can tune the vocal track under simple conditions, such as a sustained chord. 
\textit{Stage Five---Process capability}
You can control the variables with precision across a range of values and develop a consistent ``recipe''. You develop an automatic pitch correction system.
\textit{Stage Six---Process characterization} (know how)
You know how the variable affects the result and can fine-tune your ``recipe''. You make adjustments to your autotuner.
\textit{Stage Seven---Know why}
You have a scientific model of the process. You have a theory of musical intonation. 
\textit{Stage Eight---Complete knowledge}
You know the ``complete functional form and parameter values that determine the result [...] as a function of all the inputs''. This stage is never reached in practice but can be approached asymptotically. You fully understand musical intonation in all its complexity and subjectivity. 

Bohn writes: ``High-tech manufacturing requires rapid learning about multiple variables in new products and processes. We can frame a definition in terms of the stage of knowledge: \textit{high-tech processes are those in which many of the important variables are at stage four or below}. This makes the process difficult to control and work with, so a lot of effort goes into raising the knowledge level as quickly as possible.'' \cite{bohn1998measuring} Bohn's description shows that much technological development happens before a theory can be formulated. What is important is the ability to measure and control the mean. 

Auto-Tune can be described as having reached Stage Five, where its recipe reliably produces a result that can be considered accurate. The measure and control of the mean stages three and four are well defined but limited by powerful assumptions about musical intonation. My approach in this thesis is to move down the stages of knowledge, to update awareness, measure, and control of the mean. In the process, I also reassess the current technical capacity to build new tools, which is evolving rapidly according to Moore's law. 

Moving up and down the stages of knowledge has been a consistent theme in the history of music. For example, limiting musical intervals to the \textit{pure} octave and fifth in the Middle Ages provided the basis for development of Western harmony. But this development happened when musicians questioned being limited to two intervals and, ultimately, added more notes to the scale. Another example is that of keyboard instruments, which were developed for Classical music, but were not sturdy enough for Beethoven's compositions. Instrument makers had to update their technology to accommodate the composer's artistic expression. In order to use engineering in service of art, I think it is important to continue in the tradition of moving up and down the stages of knowledge.

\subsection{Pre-theory research}
\label{sec:pretheory}
Hambrick writes about the usefulness of research that is not ``theory-driven'' and can be defined as ``pre-theory''. It might still be ``theoretically interesting''---if it results in development or revision of existing theories. He continues by describing how a ``theory focus prevents reporting of rich detail about interesting phenomena for which no theory yet exists.'' He writes: ``Theories help us organize our thoughts, generate coherent explanations, and improve our predictions. In short, theories help us achieve understanding. But theories are not ends in themselves.'' The end goal is understanding.
Hambrick considers descriptive, empirical research as ``often particularly rich and thus useful as grounds for theory building.'' \cite{hambrick2007field}

I believe this idea from social science applies to music technology. Music is not considered something that we, as humans, understand deeply, despite the mature field of music theory and of the existence of multiple theories that address various aspects of how music works. This has not stopped humanity from developing musical instruments and making music. Continuing this tradition in music technology might lead to new means of musical expression, new developments in music theory, or both of the above.

\section{Developing music technology: Control, interpretability, and expressivity}
When developing music technology in practice, choices need to be made regarding how much control the programmer has over the output, how interpretable the program's actions are, and how complex functions the program can express. These features tend to come at the expense of each other. In this section, I describe the advantages and disadvantages of increasing each of these characteristics, and where I choose to place the proposed program. 

\subsection{Control}
Programmers usually provide a computer with exact, step-by-step instructions. The computer then executes these exactly, with no ability to deal with ambiguity. When designing music technology, a natural, naive approach is to provide the computer precise instructions for what to output under a comprehensive list of circumstances. This approach provides exact control over the output. However, formulating this set of instructions is quite similar to formulating a theory. As described in \ref{sec:theory}, this tends to result in simplifications and loss of richness. 

Developments in machine learning and deep learning have provided a framework for inserting randomness into music technology. Specifically, they utilize computers' capacity to generate pseudorandom numbers and to follow statistical distributions. They also provide a framework for computers to detect or learn patterns in data. Incorporating randomness into music technology removes some control. Music Plus One \cite{raphael2010music}, a musical accompaniment system, provides an example. The accompaniment track dynamically adjusts tempo based on a soloist's actions. The adjustments are based on real-time score matching using a hidden Markov model and future note timing prediction based on a Kalman filter-like model. Though the structure of these models is precisely determined by the programmer, and therefore controlled, the actual values adopted during a performance are out of the programmer's control. The result is much more dynamic than an alternative, where the programmer would have enforced specific tempi under specific circumstances. I note that Auto-Tune is one example of a program that provides full control and no randomness.

Other machine-learning programs provide less control than Music Plus One. Non-negative matrix factorization (NMF) \cite{LeeDD2000nips}---e.g., for magnitude spectrogram processing---is one such example. A NMF program represents a matrix as a set of basis vectors and activations. It iteratively minimizes the error between the input and reconstruction. The outcome is partially under the programmer's control, as the program enforces desired characteristics such as non-negativity. However, the model can converge to multiple different results, and this aspect cannot be pre-determined. A deep neural network (DNN) is by default even less controllable. The programmer can design the input and target data to the program and train the model to output data that is similar to the target, but has little control over the weights that the model learns. The programmer also has little control over what the program will output given previously unseen input data. In future sections, I describe how structuring a DNN and regularization can restore some amount of control. 

\subsection{Interpretability}
\label{sec:interpretability}
When weights learned by the program can be explained---connected to understandable features such as fundamental frequency or spectrogram amplitude---a program is interpretable. A linear model is one example of such a program. A ``black box'' program, where the weights take on an abstract meaning, is difficult to interpret. 

Interpretability is often desirable when developing machine or deep learning models, as described by Molnar \cite{molnar2020interpretable}. Interpretability makes it easier to understand why the program fails under certain conditions. This leads to the ability to debug and to build safety measures. Interpretability also can be used to ensure only relevant features are used for predictions, building reliability addressing problems such as bias against minorities. Furthermore, it can build trust among users, especially when stakes are high, e.g., in the medical field. Finally, it makes it possible to ask Why instead of What, leading towards development of new theories. 

Interpretability is not as crucial when the stakes are low or when the problem is well studied. One example of such a program is optical character recognition, where there is enough practical experience with the model and problems have been addressed over time. Furthermore, even in the case of models that are not directly interpretable, other techniques are being developed to methodically detect and remove bias e.g., \cite{jiang2019identifying}.

\subsection{Expressivity}
A third concept useful for characterizing a machine or deep learning program for music technology is its expressivity. The model's structural properties determine which functions it is able to compute. In the case of DNNs, complexity of the computed function has been shown to grow exponentially with depth \cite{raghu2017expressive}. More generally, higher expressivity can come at the expense of some control and interpretability. Wager describes how machine learning can be used to avoid extraneous modeling assumptions in causal inference. \cite{wager2019causal} Approaches using easily interpretable models (e.g., linear regression), often make strong but powerful assumptions. Some assumptions, though, are neither scientifically nor methodologically motivated. A partially linear model is more expressive and enables addressing more complex situations such as treatment heterogeneity, which would have been outside the family of functions expressible by the linear model. Its parameters can be estimated using generic machine learning tools. 

Though more expressive models are not always interpretable, increasing interpretability is an active research area. The boundary between understandable and ``black box'' models is not fixed. Examples include techniques for examining what deep convolutional neural networks learn e.g., by visualizing inputs that maximize the activation of the filters \cite{qin2018convolutional}; restructuring of convolutional neural networks as probabilistic models \cite{patel2016probabilistic}; and research on interpretable parameters in reinforcement learning \cite{verma2018programmatically}. 

\subsection{Where the proposed approach stands}
The proposed deep neural network autotuning provides a relatively low amount of control or interpretability, but is much more expressive than existing approaches. 

In \ref{sec:pretheory}, I briefly argue that a high level of control over exactly what should happen musically under a comprehensive list of circumstances is not necessary for music making. It might actually be misaligned with how even top musicians perform. Though a top musician has technical mastery of his or her musical instrument, much randomness can occur during a performance, and the musician is reacting spontaneously to other musicians' actions and other factors such as the mood they are in. I assume that such reactions are not fully controlled by the musician. Furthermore, looking back, the musician is likely not able to comprehensively interpret which factors led to specific successful or unsuccessful artistic decisions. 

As I describe in more detail throughout the thesis, the proposed approach is more expressive than Auto-Tune. Reasons include the fact that it can predict pitch shifts to any frequency on the continuous scale instead of to a discrete value; that it can be trained to apply to musical genres and cultures that do not use the equal-tempered scale; that it utilizes vocals and backing track timbre instead of measured vocals fundamental frequency to make predictions. The proposed model is by far not the most expressive one could develop. For example, it predicts a constant shift for every note instead of a continuous, frame-by-frame shift. This design decision increases control over the output audio by severely restricting how it can be modified. 

\subsubsection{What happens when the program fails?}
As described in \ref{sec:interpretability}, it is important to consider what types of erroneous output or bias the program might produce, consider how to prevent it, and determine how much to prioritize minimizing these wrong outputs compared to expressivity. Example where avoiding wrong outputs is the top priority is the design of self-driving cars, where a physical risk is involved, or a credit score predictor, where bias can impact an individual's economic status. 

The effect of an automatic pitch correction depends on the context in which it is used. The context for the proposed program is as a post-processing tool for an amateur, karaoke-style performance. If the user applies the plug-in, and a note gets worse, I imagine two negative outcomes. One is that the user notice the mistake and decide not to use the plug-in, even if it might improve other notes. Another is that a user with low confidence in his or her ability to hear pitch would assume the program is correct, and as a result feel less confident or accept a bad result. However, a solution to both negative outcomes would be to design the program to be interactive, and make clear to the user that, as a machine-learning model, the autotuner sometimes makes mistakes. The interaction would, for example, let the user listen to ``before'' and ``after'' segments and decide note by note whether or not to accept the result. Optionally, the user could even adjust the prediction. Framed in this way, I imagine that the program might actually serve as an ear training tool, which lets singers refine their ability to hear subtle pitch shifts and how these affect intonation. With excellent design, detecting mistakes might actually make the program more fun to interact with and musically stimulating than a program that enforces its model of in-tune singing on the user's performance and ``considers'' itself to always be correct. Ideally, the program could even learn from user feedback to avoid repeating mistakes and become tailored to individual preferences and styles.

As I describe in more detail in later chapters, bias can an issue for this program, especially in the context of less common musical genres. Training a model on pop will likely make it unuseable for Classical Indian music or blues because the singing styles are very different. Just as singers develop a style within a specific musical style, and build on a specific musical culture, the model should be trained on the appropriate dataset. It is also important to include a variety of vocal timbres and even accents as vowel formant affects timbre. 

What becomes clear is that, in deploying the proposed autotuner, it would be important to involve professional designers and/or music theorists or musicologists to ensure a positive user experience and to make sure that musical genres and singing styles are properly represented.

\subsection{Why I choose deep learning}
As mentioned in the introduction to this chapter, there are good reasons to move beyond MIDI for music representation. In the field of audio processing, deep learning has proven to be a technology that lets us do so, for example, by representing audio at the level of the sample or spectrogram bin. The software and hardware developed for deep learning tasks are powerful enough to process audio data in its full complexity and preserve its richness of audio. Deep learning methods have been shown to produce impressive, and state-of-the-art results on ill-defined audio tasks such as generating speech for home assistants \cite{oord2016wavenet}, pitch detection \cite{bittner2017deep} or dereverberation \cite{su2020hifi}. 